{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "from gym import error, spaces\n",
    "from gym import utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "try:\n",
    "    import atari_py\n",
    "except ImportError as e:\n",
    "    raise error.DependencyNotInstalled(\"{}. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def eps_greedy_policy(q_values, eps):\n",
    "    '''\n",
    "    Creates an epsilon-greedy policy\n",
    "    :param q_values: set of Q-values of shape (num actions,)\n",
    "    :param eps: probability of taking a uniform random action \n",
    "    :return: policy of shape (num actions,)\n",
    "    '''\n",
    "\n",
    "    policy = np.zeros_like(q_values)\n",
    "    r = random.uniform(0,1)\n",
    "    \n",
    "    if(r < eps):\n",
    "        policy[:] = 1./len(q_values)\n",
    "    else:\n",
    "        greedy_action = np.argmax(q_values)\n",
    "        policy[greedy_action] = 1.\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def calculate_td_targets(q1_batch, q2_batch, r_batch, t_batch, gamma=.99):\n",
    "    '''\n",
    "    Calculates the TD-target used for the loss\n",
    "    : param q1_batch: Batch of Q(s', a) from online network, shape (N, num actions)\n",
    "    : param q2_batch: Batch of Q(s', a) from target network, shape (N, num actions)\n",
    "    : param r_batch: Batch of rewards, shape (N, 1)\n",
    "    : param t_batch: Batch of booleans indicating if state, s' is terminal, shape (N, 1)\n",
    "    : return: TD-target, shape (N, 1)\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    Y = np.zeros_like(r_batch)\n",
    "    \n",
    "    for i,(q1, q2, r, t) in enumerate(zip(q1_batch, q2_batch, r_batch, t_batch)):\n",
    "        Y[i] = r\n",
    "        if not t: Y[i] += gamma * q2[np.argmax(q1)]\n",
    "        \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ram(ale):\n",
    "    ram_size = ale.getRAMSize()\n",
    "    ram = np.zeros((ram_size),dtype=np.uint8)\n",
    "    ale.getRAM(ram)\n",
    "    return ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from keras.utils.np_utils import to_categorical as one_hot\n",
    "from collections import namedtuple\n",
    "from dqn_model import DoubleQLearningModel, ExperienceReplay\n",
    "\n",
    "def train_loop_ddqn(model, env, num_episodes, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .0005\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() #reset to initial state\n",
    "        state = np.expand_dims(state, axis=0)/2\n",
    "        terminal = False # reset terminal flag\n",
    "        ep_reward = 0\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not terminal:\n",
    "            env.render() # comment this line out if jous' don't want to render the environment\n",
    "            steps += 1\n",
    "            q_values = model.get_q_values(state)\n",
    "            q_buffer.append(q_values)\n",
    "            policy = eps_greedy_policy(q_values.squeeze(), eps) \n",
    "            action = np.random.choice(num_actions, p=policy) # sample action from epsilon-greedy policy\n",
    "            new_state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "            new_state = np.expand_dims(new_state, axis=0)/2\n",
    "            \n",
    "            # only use the terminal flag for ending the episode and not for training\n",
    "            # if the flag is set due to that the maximum amount of steps is reached \n",
    "            t_to_buffer = terminal if not steps == 200 else False\n",
    "            \n",
    "            # store data to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=action, r=reward, next_s=new_state, t=t_to_buffer))\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # if buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                s, a, r, s_, t = replay_buffer.sample_minibatch(batch_size) # sample a minibatch of transitions\n",
    "                q_1, q_2 = model.get_q_values_for_both_models(np.squeeze(s_))\n",
    "                td_target = calculate_td_targets(q_1, q_2, r, t, gamma)\n",
    "                model.update(s, td_target, a)    \n",
    "                \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # running average of episodic rewards\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "        print('Episode: ', i, 'Reward:', ep_reward, 'Epsilon', eps, 'mean q', np.mean(np.array(q_buffer)))\n",
    "        \n",
    "        # if running average > 195, the task is considerd solved\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariEnv(gym.Env, utils.EzPickle):\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "\n",
    "    def __init__(self, game='SpaceInvaders-v0', obs_type='ram', frameskip=(2, 5), repeat_action_probability=0.):\n",
    "        \"\"\"Frameskip should be either a tuple (indicating a random range to\n",
    "        choose from, with the top value exclude), or an int.\"\"\"\n",
    "\n",
    "        utils.EzPickle.__init__(self, game, obs_type, frameskip, repeat_action_probability)\n",
    "        assert obs_type in ('ram', 'image')\n",
    "\n",
    "        self.game_path = atari_py.get_game_path(game)\n",
    "        if not os.path.exists(self.game_path):\n",
    "            raise IOError('You asked for game %s but path %s does not exist'%(game, self.game_path))\n",
    "        self._obs_type = obs_type\n",
    "        self.frameskip = frameskip\n",
    "        self.ale = atari_py.ALEInterface()\n",
    "        self.viewer = None\n",
    "\n",
    "        # Tune (or disable) ALE's action repeat:\n",
    "        # https://github.com/openai/gym/issues/349\n",
    "        assert isinstance(repeat_action_probability, (float, int)), \"Invalid repeat_action_probability: {!r}\".format(repeat_action_probability)\n",
    "        self.ale.setFloat('repeat_action_probability'.encode('utf-8'), repeat_action_probability)\n",
    "\n",
    "        self.seed()\n",
    "\n",
    "        self._action_set = self.ale.getMinimalActionSet()\n",
    "        self.action_space = spaces.Discrete(len(self._action_set))\n",
    "\n",
    "        (screen_width,screen_height) = self.ale.getScreenDims()\n",
    "        if self._obs_type == 'ram':\n",
    "            self.observation_space = spaces.Box(low=0, high=255, dtype=np.uint8, shape=(128,))\n",
    "        elif self._obs_type == 'image':\n",
    "            self.observation_space = spaces.Box(low=0, high=255, shape=(screen_height, screen_width, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            raise error.Error('Unrecognized observation type: {}'.format(self._obs_type))\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed1 = seeding.np_random(seed)\n",
    "        # Derive a random seed. This gets passed as a uint, but gets\n",
    "        # checked as an int elsewhere, so we need to keep it below\n",
    "        # 2**31.\n",
    "        seed2 = seeding.hash_seed(seed1 + 1) % 2**31\n",
    "        # Empirically, we need to seed before loading the ROM.\n",
    "        self.ale.setInt(b'random_seed', seed2)\n",
    "        self.ale.loadROM(self.game_path)\n",
    "        return [seed1, seed2]\n",
    "\n",
    "    def step(self, a):\n",
    "        reward = 0.0\n",
    "        action = self._action_set[a]\n",
    "\n",
    "        if isinstance(self.frameskip, int):\n",
    "            num_steps = self.frameskip\n",
    "        else:\n",
    "            num_steps = self.np_random.randint(self.frameskip[0], self.frameskip[1])\n",
    "        for _ in range(num_steps):\n",
    "            reward += self.ale.act(action)\n",
    "        ob = self._get_obs()\n",
    "\n",
    "        return ob, reward, self.ale.game_over(), {\"ale.lives\": self.ale.lives()}\n",
    "\n",
    "    def _get_image(self):\n",
    "        return self.ale.getScreenRGB2()\n",
    "\n",
    "    def _get_ram(self):\n",
    "        return to_ram(self.ale)\n",
    "\n",
    "    @property\n",
    "    def _n_actions(self):\n",
    "        return len(self._action_set)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self._obs_type == 'ram':\n",
    "            return self._get_ram()\n",
    "        elif self._obs_type == 'image':\n",
    "            img = self._get_image()\n",
    "        return img\n",
    "\n",
    "    # return: (states, observations)\n",
    "    def reset(self):\n",
    "        self.ale.reset_game()\n",
    "        return self._get_obs()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        img = self._get_image()\n",
    "        if mode == 'rgb_array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            from gym.envs.classic_control import rendering\n",
    "            if self.viewer is None:\n",
    "                self.viewer = rendering.SimpleImageViewer()\n",
    "            self.viewer.imshow(img)\n",
    "            return self.viewer.isopen\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "    def get_action_meanings(self):\n",
    "        return [ACTION_MEANING[i] for i in self._action_set]\n",
    "\n",
    "    def get_keys_to_action(self):\n",
    "        KEYWORD_TO_KEY = {\n",
    "            'UP':      ord('w'),\n",
    "            'DOWN':    ord('s'),\n",
    "            'LEFT':    ord('a'),\n",
    "            'RIGHT':   ord('d'),\n",
    "            'FIRE':    ord(' '),\n",
    "        }\n",
    "\n",
    "        keys_to_action = {}\n",
    "\n",
    "        for action_id, action_meaning in enumerate(self.get_action_meanings()):\n",
    "            keys = []\n",
    "            for keyword, key in KEYWORD_TO_KEY.items():\n",
    "                if keyword in action_meaning:\n",
    "                    keys.append(key)\n",
    "            keys = tuple(sorted(keys))\n",
    "\n",
    "            assert keys not in keys_to_action\n",
    "            keys_to_action[keys] = action_id\n",
    "\n",
    "        return keys_to_action\n",
    "\n",
    "    def clone_state(self):\n",
    "        \"\"\"Clone emulator state w/o system state. Restoring this state will\n",
    "        *not* give an identical environment. For complete cloning and restoring\n",
    "        of the full state, see `{clone,restore}_full_state()`.\"\"\"\n",
    "        state_ref = self.ale.cloneState()\n",
    "        state = self.ale.encodeState(state_ref)\n",
    "        self.ale.deleteState(state_ref)\n",
    "        return state\n",
    "\n",
    "    def restore_state(self, state):\n",
    "        \"\"\"Restore emulator state w/o system state.\"\"\"\n",
    "        state_ref = self.ale.decodeState(state)\n",
    "        self.ale.restoreState(state_ref)\n",
    "        self.ale.deleteState(state_ref)\n",
    "\n",
    "    def clone_full_state(self):\n",
    "        \"\"\"Clone emulator state w/ system state including pseudorandomness.\n",
    "        Restoring this state will give an identical environment.\"\"\"\n",
    "        state_ref = self.ale.cloneSystemState()\n",
    "        state = self.ale.encodeState(state_ref)\n",
    "        self.ale.deleteState(state_ref)\n",
    "        return state\n",
    "\n",
    "    def restore_full_state(self, state):\n",
    "        \"\"\"Restore emulator state w/ system state including pseudorandomness.\"\"\"\n",
    "        state_ref = self.ale.decodeState(state)\n",
    "        self.ale.restoreSystemState(state_ref)\n",
    "        self.ale.deleteState(state_ref)\n",
    "\n",
    "ACTION_MEANING = {\n",
    "    0 : \"NOOP\",\n",
    "    1 : \"FIRE\",\n",
    "    2 : \"UP\",\n",
    "    3 : \"RIGHT\",\n",
    "    4 : \"LEFT\",\n",
    "    5 : \"DOWN\",\n",
    "    6 : \"UPRIGHT\",\n",
    "    7 : \"UPLEFT\",\n",
    "    8 : \"DOWNRIGHT\",\n",
    "    9 : \"DOWNLEFT\",\n",
    "    10 : \"UPFIRE\",\n",
    "    11 : \"RIGHTFIRE\",\n",
    "    12 : \"LEFTFIRE\",\n",
    "    13 : \"DOWNFIRE\",\n",
    "    14 : \"UPRIGHTFIRE\",\n",
    "    15 : \"UPLEFTFIRE\",\n",
    "    16 : \"DOWNRIGHTFIRE\",\n",
    "    17 : \"DOWNLEFTFIRE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 Reward: 75.0 Epsilon 0.9995 mean q -1.635741e-05\n",
      "Episode:  1 Reward: 105.0 Epsilon 0.9990000000000001 mean q 0.019506026\n",
      "Episode:  2 Reward: 50.0 Epsilon 0.9985000000000002 mean q 3.3235056\n",
      "Episode:  3 Reward: 30.0 Epsilon 0.9980000000000002 mean q 5.3060365\n",
      "Episode:  4 Reward: 75.0 Epsilon 0.9975000000000003 mean q 4.168429\n",
      "Episode:  5 Reward: 135.0 Epsilon 0.9970000000000003 mean q 5.3198485\n",
      "Episode:  6 Reward: 90.0 Epsilon 0.9965000000000004 mean q 6.465825\n",
      "Episode:  7 Reward: 105.0 Epsilon 0.9960000000000004 mean q 5.1163793\n",
      "Episode:  8 Reward: 210.0 Epsilon 0.9955000000000005 mean q 6.548742\n",
      "Episode:  9 Reward: 105.0 Epsilon 0.9950000000000006 mean q 4.9635496\n",
      "Episode:  10 Reward: 135.0 Epsilon 0.9945000000000006 mean q 2.7467902\n",
      "Episode:  11 Reward: 110.0 Epsilon 0.9940000000000007 mean q 3.3210301\n",
      "Episode:  12 Reward: 245.0 Epsilon 0.9935000000000007 mean q 6.7373\n",
      "Episode:  13 Reward: 105.0 Epsilon 0.9930000000000008 mean q 4.789228\n",
      "Episode:  14 Reward: 45.0 Epsilon 0.9925000000000008 mean q 5.6545396\n",
      "Episode:  15 Reward: 85.0 Epsilon 0.9920000000000009 mean q 4.1209283\n",
      "Episode:  16 Reward: 50.0 Epsilon 0.9915000000000009 mean q 5.5627203\n",
      "Episode:  17 Reward: 35.0 Epsilon 0.991000000000001 mean q 4.9229293\n",
      "Episode:  18 Reward: 110.0 Epsilon 0.990500000000001 mean q 5.9772077\n",
      "Episode:  19 Reward: 155.0 Epsilon 0.9900000000000011 mean q 4.937222\n",
      "Episode:  20 Reward: 510.0 Epsilon 0.9895000000000012 mean q 5.718841\n",
      "Episode:  21 Reward: 110.0 Epsilon 0.9890000000000012 mean q 6.715014\n",
      "Episode:  22 Reward: 80.0 Epsilon 0.9885000000000013 mean q 2.7088203\n",
      "Episode:  23 Reward: 345.0 Epsilon 0.9880000000000013 mean q 4.515618\n",
      "Episode:  24 Reward: 215.0 Epsilon 0.9875000000000014 mean q 6.1399946\n",
      "Episode:  25 Reward: 105.0 Epsilon 0.9870000000000014 mean q 5.0123796\n",
      "Episode:  26 Reward: 80.0 Epsilon 0.9865000000000015 mean q 4.513102\n",
      "Episode:  27 Reward: 45.0 Epsilon 0.9860000000000015 mean q 3.0806055\n",
      "Episode:  28 Reward: 45.0 Epsilon 0.9855000000000016 mean q 3.8249478\n",
      "Episode:  29 Reward: 65.0 Epsilon 0.9850000000000017 mean q 2.9216316\n",
      "Episode:  30 Reward: 180.0 Epsilon 0.9845000000000017 mean q 5.1691594\n",
      "Episode:  31 Reward: 215.0 Epsilon 0.9840000000000018 mean q 4.0983367\n",
      "Episode:  32 Reward: 265.0 Epsilon 0.9835000000000018 mean q 5.0608277\n",
      "Episode:  33 Reward: 125.0 Epsilon 0.9830000000000019 mean q 4.950109\n",
      "Episode:  34 Reward: 105.0 Epsilon 0.9825000000000019 mean q 5.089549\n",
      "Episode:  35 Reward: 105.0 Epsilon 0.982000000000002 mean q 4.239309\n",
      "Episode:  36 Reward: 40.0 Epsilon 0.981500000000002 mean q 4.229792\n",
      "Episode:  37 Reward: 85.0 Epsilon 0.9810000000000021 mean q 4.1405907\n",
      "Episode:  38 Reward: 135.0 Epsilon 0.9805000000000021 mean q 4.584525\n",
      "Episode:  39 Reward: 515.0 Epsilon 0.9800000000000022 mean q 5.086406\n",
      "Episode:  40 Reward: 110.0 Epsilon 0.9795000000000023 mean q 5.3078074\n",
      "Episode:  41 Reward: 155.0 Epsilon 0.9790000000000023 mean q 4.905771\n",
      "Episode:  42 Reward: 75.0 Epsilon 0.9785000000000024 mean q 2.3193889\n",
      "Episode:  43 Reward: 75.0 Epsilon 0.9780000000000024 mean q 2.56177\n",
      "Episode:  44 Reward: 30.0 Epsilon 0.9775000000000025 mean q 3.5454068\n",
      "Episode:  45 Reward: 155.0 Epsilon 0.9770000000000025 mean q 5.7415724\n",
      "Episode:  46 Reward: 135.0 Epsilon 0.9765000000000026 mean q 4.749433\n",
      "Episode:  47 Reward: 110.0 Epsilon 0.9760000000000026 mean q 3.2381694\n",
      "Episode:  48 Reward: 135.0 Epsilon 0.9755000000000027 mean q 4.818234\n",
      "Episode:  49 Reward: 105.0 Epsilon 0.9750000000000028 mean q 3.561295\n",
      "Episode:  50 Reward: 290.0 Epsilon 0.9745000000000028 mean q 4.5161905\n",
      "Episode:  51 Reward: 120.0 Epsilon 0.9740000000000029 mean q 3.3060958\n",
      "Episode:  52 Reward: 135.0 Epsilon 0.9735000000000029 mean q 3.331834\n",
      "Episode:  53 Reward: 135.0 Epsilon 0.973000000000003 mean q 3.4344094\n",
      "Episode:  54 Reward: 135.0 Epsilon 0.972500000000003 mean q 3.194607\n",
      "Episode:  55 Reward: 45.0 Epsilon 0.9720000000000031 mean q 3.1245048\n",
      "Episode:  56 Reward: 75.0 Epsilon 0.9715000000000031 mean q 3.7386208\n",
      "Episode:  57 Reward: 160.0 Epsilon 0.9710000000000032 mean q 4.477273\n",
      "Episode:  58 Reward: 110.0 Epsilon 0.9705000000000032 mean q 3.994521\n",
      "Episode:  59 Reward: 150.0 Epsilon 0.9700000000000033 mean q 3.8995717\n",
      "Episode:  60 Reward: 105.0 Epsilon 0.9695000000000034 mean q 3.4682002\n",
      "Episode:  61 Reward: 105.0 Epsilon 0.9690000000000034 mean q 3.8080132\n",
      "Episode:  62 Reward: 385.0 Epsilon 0.9685000000000035 mean q 3.5024\n",
      "Episode:  63 Reward: 60.0 Epsilon 0.9680000000000035 mean q 3.8097577\n",
      "Episode:  64 Reward: 65.0 Epsilon 0.9675000000000036 mean q 2.7556496\n",
      "Episode:  65 Reward: 10.0 Epsilon 0.9670000000000036 mean q 2.251141\n",
      "Episode:  66 Reward: 230.0 Epsilon 0.9665000000000037 mean q 3.0455554\n",
      "Episode:  67 Reward: 75.0 Epsilon 0.9660000000000037 mean q 3.2975097\n",
      "Episode:  68 Reward: 165.0 Epsilon 0.9655000000000038 mean q 4.492118\n",
      "Episode:  69 Reward: 165.0 Epsilon 0.9650000000000039 mean q 3.569625\n",
      "Episode:  70 Reward: 50.0 Epsilon 0.9645000000000039 mean q 3.2823687\n",
      "Episode:  71 Reward: 75.0 Epsilon 0.964000000000004 mean q 3.5590885\n",
      "Episode:  72 Reward: 215.0 Epsilon 0.963500000000004 mean q 3.400582\n",
      "Episode:  73 Reward: 125.0 Epsilon 0.9630000000000041 mean q 4.374916\n",
      "Episode:  74 Reward: 435.0 Epsilon 0.9625000000000041 mean q 3.62365\n",
      "Episode:  75 Reward: 50.0 Epsilon 0.9620000000000042 mean q 4.015514\n",
      "Episode:  76 Reward: 380.0 Epsilon 0.9615000000000042 mean q 4.570723\n",
      "Episode:  77 Reward: 230.0 Epsilon 0.9610000000000043 mean q 4.081778\n",
      "Episode:  78 Reward: 110.0 Epsilon 0.9605000000000044 mean q 4.482563\n",
      "Episode:  79 Reward: 75.0 Epsilon 0.9600000000000044 mean q 4.212137\n",
      "Episode:  80 Reward: 80.0 Epsilon 0.9595000000000045 mean q 2.941901\n",
      "Episode:  81 Reward: 260.0 Epsilon 0.9590000000000045 mean q 4.099337\n",
      "Episode:  82 Reward: 105.0 Epsilon 0.9585000000000046 mean q 2.678419\n",
      "Episode:  83 Reward: 65.0 Epsilon 0.9580000000000046 mean q 4.2181225\n",
      "Episode:  84 Reward: 130.0 Epsilon 0.9575000000000047 mean q 4.608416\n",
      "Episode:  85 Reward: 380.0 Epsilon 0.9570000000000047 mean q 4.282717\n",
      "Episode:  86 Reward: 110.0 Epsilon 0.9565000000000048 mean q 3.4476473\n",
      "Episode:  87 Reward: 150.0 Epsilon 0.9560000000000048 mean q 4.770879\n",
      "Episode:  88 Reward: 60.0 Epsilon 0.9555000000000049 mean q 3.7682178\n",
      "Episode:  89 Reward: 135.0 Epsilon 0.955000000000005 mean q 3.2102692\n",
      "Episode:  90 Reward: 185.0 Epsilon 0.954500000000005 mean q 3.7745373\n",
      "Episode:  91 Reward: 60.0 Epsilon 0.9540000000000051 mean q 3.1871724\n",
      "Episode:  92 Reward: 225.0 Epsilon 0.9535000000000051 mean q 3.5002444\n",
      "Episode:  93 Reward: 105.0 Epsilon 0.9530000000000052 mean q 2.8653147\n",
      "Episode:  94 Reward: 90.0 Epsilon 0.9525000000000052 mean q 3.893035\n",
      "Episode:  95 Reward: 170.0 Epsilon 0.9520000000000053 mean q 4.5729628\n",
      "Episode:  96 Reward: 155.0 Epsilon 0.9515000000000053 mean q 3.9786665\n",
      "Episode:  97 Reward: 115.0 Epsilon 0.9510000000000054 mean q 3.9952939\n",
      "Episode:  98 Reward: 45.0 Epsilon 0.9505000000000055 mean q 2.3448002\n",
      "Episode:  99 Reward: 40.0 Epsilon 0.9500000000000055 mean q 4.0644946\n",
      "Episode:  100 Reward: 135.0 Epsilon 0.9495000000000056 mean q 3.0857081\n",
      "Episode:  101 Reward: 180.0 Epsilon 0.9490000000000056 mean q 2.6948538\n",
      "Episode:  102 Reward: 50.0 Epsilon 0.9485000000000057 mean q 3.5424488\n",
      "Episode:  103 Reward: 120.0 Epsilon 0.9480000000000057 mean q 3.3866668\n",
      "Episode:  104 Reward: 105.0 Epsilon 0.9475000000000058 mean q 3.3565779\n",
      "Episode:  105 Reward: 110.0 Epsilon 0.9470000000000058 mean q 2.6249774\n",
      "Episode:  106 Reward: 50.0 Epsilon 0.9465000000000059 mean q 2.640012\n",
      "Episode:  107 Reward: 220.0 Epsilon 0.946000000000006 mean q 2.8809485\n",
      "Episode:  108 Reward: 35.0 Epsilon 0.945500000000006 mean q 2.5175865\n",
      "Episode:  109 Reward: 250.0 Epsilon 0.9450000000000061 mean q 3.2949274\n",
      "Episode:  110 Reward: 120.0 Epsilon 0.9445000000000061 mean q 3.1688309\n",
      "Episode:  111 Reward: 190.0 Epsilon 0.9440000000000062 mean q 4.2962184\n",
      "Episode:  112 Reward: 180.0 Epsilon 0.9435000000000062 mean q 3.6789346\n",
      "Episode:  113 Reward: 375.0 Epsilon 0.9430000000000063 mean q 4.4532213\n",
      "Episode:  114 Reward: 180.0 Epsilon 0.9425000000000063 mean q 3.3941233\n",
      "Episode:  115 Reward: 105.0 Epsilon 0.9420000000000064 mean q 4.0709634\n",
      "Episode:  116 Reward: 160.0 Epsilon 0.9415000000000064 mean q 3.7538676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  117 Reward: 90.0 Epsilon 0.9410000000000065 mean q 4.2474623\n",
      "Episode:  118 Reward: 45.0 Epsilon 0.9405000000000066 mean q 2.695311\n",
      "Episode:  119 Reward: 80.0 Epsilon 0.9400000000000066 mean q 2.003827\n",
      "Episode:  120 Reward: 270.0 Epsilon 0.9395000000000067 mean q 4.224215\n",
      "Episode:  121 Reward: 180.0 Epsilon 0.9390000000000067 mean q 4.53043\n",
      "Episode:  122 Reward: 110.0 Epsilon 0.9385000000000068 mean q 3.9871812\n",
      "Episode:  123 Reward: 90.0 Epsilon 0.9380000000000068 mean q 3.9654565\n",
      "Episode:  124 Reward: 180.0 Epsilon 0.9375000000000069 mean q 4.154578\n",
      "Episode:  125 Reward: 60.0 Epsilon 0.9370000000000069 mean q 2.8047628\n",
      "Episode:  126 Reward: 60.0 Epsilon 0.936500000000007 mean q 3.8609192\n",
      "Episode:  127 Reward: 180.0 Epsilon 0.936000000000007 mean q 4.38183\n",
      "Episode:  128 Reward: 150.0 Epsilon 0.9355000000000071 mean q 4.206466\n",
      "Episode:  129 Reward: 205.0 Epsilon 0.9350000000000072 mean q 3.0707884\n",
      "Episode:  130 Reward: 140.0 Epsilon 0.9345000000000072 mean q 3.8382564\n",
      "Episode:  131 Reward: 230.0 Epsilon 0.9340000000000073 mean q 3.6526318\n",
      "Episode:  132 Reward: 105.0 Epsilon 0.9335000000000073 mean q 3.138581\n",
      "Episode:  133 Reward: 100.0 Epsilon 0.9330000000000074 mean q 3.922763\n",
      "Episode:  134 Reward: 185.0 Epsilon 0.9325000000000074 mean q 3.301648\n",
      "Episode:  135 Reward: 280.0 Epsilon 0.9320000000000075 mean q 3.607267\n",
      "Episode:  136 Reward: 135.0 Epsilon 0.9315000000000075 mean q 3.5870833\n",
      "Episode:  137 Reward: 55.0 Epsilon 0.9310000000000076 mean q 3.2924254\n",
      "Episode:  138 Reward: 140.0 Epsilon 0.9305000000000077 mean q 3.5908127\n",
      "Episode:  139 Reward: 90.0 Epsilon 0.9300000000000077 mean q 2.3036487\n",
      "Episode:  140 Reward: 120.0 Epsilon 0.9295000000000078 mean q 3.4241245\n",
      "Episode:  141 Reward: 105.0 Epsilon 0.9290000000000078 mean q 3.7165751\n",
      "Episode:  142 Reward: 160.0 Epsilon 0.9285000000000079 mean q 3.611848\n",
      "Episode:  143 Reward: 110.0 Epsilon 0.9280000000000079 mean q 3.02413\n",
      "Episode:  144 Reward: 90.0 Epsilon 0.927500000000008 mean q 3.9403458\n",
      "Episode:  145 Reward: 145.0 Epsilon 0.927000000000008 mean q 3.5916123\n",
      "Episode:  146 Reward: 55.0 Epsilon 0.9265000000000081 mean q 2.8044817\n",
      "Episode:  147 Reward: 150.0 Epsilon 0.9260000000000081 mean q 3.6972778\n",
      "Episode:  148 Reward: 210.0 Epsilon 0.9255000000000082 mean q 3.1398952\n",
      "Episode:  149 Reward: 30.0 Epsilon 0.9250000000000083 mean q 2.9772015\n",
      "Episode:  150 Reward: 245.0 Epsilon 0.9245000000000083 mean q 2.9251664\n",
      "Episode:  151 Reward: 345.0 Epsilon 0.9240000000000084 mean q 3.715445\n",
      "Episode:  152 Reward: 105.0 Epsilon 0.9235000000000084 mean q 2.6388078\n",
      "Episode:  153 Reward: 180.0 Epsilon 0.9230000000000085 mean q 3.6635046\n",
      "Episode:  154 Reward: 75.0 Epsilon 0.9225000000000085 mean q 2.5749037\n",
      "Episode:  155 Reward: 110.0 Epsilon 0.9220000000000086 mean q 2.995537\n",
      "Episode:  156 Reward: 65.0 Epsilon 0.9215000000000086 mean q 3.2644188\n",
      "Episode:  157 Reward: 185.0 Epsilon 0.9210000000000087 mean q 4.176815\n",
      "Episode:  158 Reward: 80.0 Epsilon 0.9205000000000088 mean q 3.458701\n",
      "Episode:  159 Reward: 135.0 Epsilon 0.9200000000000088 mean q 3.5293443\n",
      "Episode:  160 Reward: 240.0 Epsilon 0.9195000000000089 mean q 3.2602506\n",
      "Episode:  161 Reward: 240.0 Epsilon 0.9190000000000089 mean q 4.5422935\n",
      "Episode:  162 Reward: 80.0 Epsilon 0.918500000000009 mean q 3.1074457\n",
      "Episode:  163 Reward: 60.0 Epsilon 0.918000000000009 mean q 2.984338\n",
      "Episode:  164 Reward: 55.0 Epsilon 0.9175000000000091 mean q 3.3773198\n",
      "Episode:  165 Reward: 45.0 Epsilon 0.9170000000000091 mean q 3.0747702\n",
      "Episode:  166 Reward: 80.0 Epsilon 0.9165000000000092 mean q 2.5830765\n",
      "Episode:  167 Reward: 105.0 Epsilon 0.9160000000000093 mean q 3.129901\n",
      "Episode:  168 Reward: 75.0 Epsilon 0.9155000000000093 mean q 2.5153027\n",
      "Episode:  169 Reward: 185.0 Epsilon 0.9150000000000094 mean q 3.5214598\n",
      "Episode:  170 Reward: 80.0 Epsilon 0.9145000000000094 mean q 2.6875985\n",
      "Episode:  171 Reward: 810.0 Epsilon 0.9140000000000095 mean q 3.1042686\n",
      "Episode:  172 Reward: 215.0 Epsilon 0.9135000000000095 mean q 3.5564928\n",
      "Episode:  173 Reward: 105.0 Epsilon 0.9130000000000096 mean q 2.9068751\n",
      "Episode:  174 Reward: 210.0 Epsilon 0.9125000000000096 mean q 3.5997615\n",
      "Episode:  175 Reward: 45.0 Epsilon 0.9120000000000097 mean q 2.2211876\n",
      "Episode:  176 Reward: 95.0 Epsilon 0.9115000000000097 mean q 3.4119015\n",
      "Episode:  177 Reward: 265.0 Epsilon 0.9110000000000098 mean q 2.4211016\n",
      "Episode:  178 Reward: 180.0 Epsilon 0.9105000000000099 mean q 3.7102506\n",
      "Episode:  179 Reward: 135.0 Epsilon 0.9100000000000099 mean q 2.8614922\n",
      "Episode:  180 Reward: 300.0 Epsilon 0.90950000000001 mean q 2.6786063\n",
      "Episode:  181 Reward: 105.0 Epsilon 0.90900000000001 mean q 1.886767\n",
      "Episode:  182 Reward: 85.0 Epsilon 0.9085000000000101 mean q 2.1038105\n",
      "Episode:  183 Reward: 410.0 Epsilon 0.9080000000000101 mean q 3.226849\n",
      "Episode:  184 Reward: 180.0 Epsilon 0.9075000000000102 mean q 3.1833515\n",
      "Episode:  185 Reward: 105.0 Epsilon 0.9070000000000102 mean q 3.088234\n",
      "Episode:  186 Reward: 130.0 Epsilon 0.9065000000000103 mean q 2.8858578\n",
      "Episode:  187 Reward: 515.0 Epsilon 0.9060000000000104 mean q 3.8335114\n",
      "Episode:  188 Reward: 90.0 Epsilon 0.9055000000000104 mean q 3.3399205\n",
      "Episode:  189 Reward: 160.0 Epsilon 0.9050000000000105 mean q 3.3360934\n",
      "Episode:  190 Reward: 210.0 Epsilon 0.9045000000000105 mean q 3.566393\n",
      "Episode:  191 Reward: 65.0 Epsilon 0.9040000000000106 mean q 3.1186392\n",
      "Episode:  192 Reward: 50.0 Epsilon 0.9035000000000106 mean q 2.0059357\n",
      "Episode:  193 Reward: 30.0 Epsilon 0.9030000000000107 mean q 2.3942347\n",
      "Episode:  194 Reward: 105.0 Epsilon 0.9025000000000107 mean q 2.702456\n",
      "Episode:  195 Reward: 155.0 Epsilon 0.9020000000000108 mean q 3.5084538\n",
      "Episode:  196 Reward: 250.0 Epsilon 0.9015000000000108 mean q 2.853463\n",
      "Episode:  197 Reward: 30.0 Epsilon 0.9010000000000109 mean q 2.4387152\n",
      "Episode:  198 Reward: 80.0 Epsilon 0.900500000000011 mean q 3.6559303\n",
      "Episode:  199 Reward: 55.0 Epsilon 0.900000000000011 mean q 2.8567207\n",
      "Episode:  200 Reward: 105.0 Epsilon 0.8995000000000111 mean q 3.1233392\n",
      "Episode:  201 Reward: 25.0 Epsilon 0.8990000000000111 mean q 2.6335623\n",
      "Episode:  202 Reward: 75.0 Epsilon 0.8985000000000112 mean q 2.6476626\n",
      "Episode:  203 Reward: 340.0 Epsilon 0.8980000000000112 mean q 2.9275784\n",
      "Episode:  204 Reward: 110.0 Epsilon 0.8975000000000113 mean q 2.9444017\n",
      "Episode:  205 Reward: 120.0 Epsilon 0.8970000000000113 mean q 3.8681853\n",
      "Episode:  206 Reward: 15.0 Epsilon 0.8965000000000114 mean q 3.231261\n",
      "Episode:  207 Reward: 150.0 Epsilon 0.8960000000000115 mean q 3.9914196\n",
      "Episode:  208 Reward: 110.0 Epsilon 0.8955000000000115 mean q 3.0941303\n",
      "Episode:  209 Reward: 410.0 Epsilon 0.8950000000000116 mean q 3.4323106\n",
      "Episode:  210 Reward: 150.0 Epsilon 0.8945000000000116 mean q 4.5789766\n",
      "Episode:  211 Reward: 215.0 Epsilon 0.8940000000000117 mean q 4.419035\n",
      "Episode:  212 Reward: 135.0 Epsilon 0.8935000000000117 mean q 3.6501763\n",
      "Episode:  213 Reward: 135.0 Epsilon 0.8930000000000118 mean q 3.1023703\n",
      "Episode:  214 Reward: 155.0 Epsilon 0.8925000000000118 mean q 3.1611402\n",
      "Episode:  215 Reward: 345.0 Epsilon 0.8920000000000119 mean q 2.9750948\n",
      "Episode:  216 Reward: 185.0 Epsilon 0.891500000000012 mean q 4.2066507\n",
      "Episode:  217 Reward: 285.0 Epsilon 0.891000000000012 mean q 4.2062106\n",
      "Episode:  218 Reward: 110.0 Epsilon 0.8905000000000121 mean q 2.3715634\n",
      "Episode:  219 Reward: 55.0 Epsilon 0.8900000000000121 mean q 3.4666924\n",
      "Episode:  220 Reward: 185.0 Epsilon 0.8895000000000122 mean q 4.0609503\n",
      "Episode:  221 Reward: 115.0 Epsilon 0.8890000000000122 mean q 3.696391\n",
      "Episode:  222 Reward: 55.0 Epsilon 0.8885000000000123 mean q 2.9461555\n",
      "Episode:  223 Reward: 210.0 Epsilon 0.8880000000000123 mean q 2.5445313\n",
      "Episode:  224 Reward: 245.0 Epsilon 0.8875000000000124 mean q 4.126757\n",
      "Episode:  225 Reward: 105.0 Epsilon 0.8870000000000124 mean q 3.448456\n",
      "Episode:  226 Reward: 30.0 Epsilon 0.8865000000000125 mean q 3.770587\n",
      "Episode:  227 Reward: 30.0 Epsilon 0.8860000000000126 mean q 2.661158\n",
      "Episode:  228 Reward: 90.0 Epsilon 0.8855000000000126 mean q 3.0507047\n",
      "Episode:  229 Reward: 335.0 Epsilon 0.8850000000000127 mean q 3.7789962\n",
      "Episode:  230 Reward: 30.0 Epsilon 0.8845000000000127 mean q 3.0877278\n",
      "Episode:  231 Reward: 110.0 Epsilon 0.8840000000000128 mean q 2.9324996\n",
      "Episode:  232 Reward: 240.0 Epsilon 0.8835000000000128 mean q 3.1448033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  233 Reward: 80.0 Epsilon 0.8830000000000129 mean q 2.3577392\n",
      "Episode:  234 Reward: 235.0 Epsilon 0.8825000000000129 mean q 3.1047587\n",
      "Episode:  235 Reward: 230.0 Epsilon 0.882000000000013 mean q 3.5488503\n",
      "Episode:  236 Reward: 210.0 Epsilon 0.881500000000013 mean q 3.3005116\n",
      "Episode:  237 Reward: 110.0 Epsilon 0.8810000000000131 mean q 2.5167751\n",
      "Episode:  238 Reward: 385.0 Epsilon 0.8805000000000132 mean q 3.2019997\n",
      "Episode:  239 Reward: 45.0 Epsilon 0.8800000000000132 mean q 2.9402237\n",
      "Episode:  240 Reward: 215.0 Epsilon 0.8795000000000133 mean q 4.415627\n",
      "Episode:  241 Reward: 110.0 Epsilon 0.8790000000000133 mean q 4.097623\n",
      "Episode:  242 Reward: 110.0 Epsilon 0.8785000000000134 mean q 2.6386933\n",
      "Episode:  243 Reward: 135.0 Epsilon 0.8780000000000134 mean q 2.78817\n",
      "Episode:  244 Reward: 100.0 Epsilon 0.8775000000000135 mean q 3.400431\n",
      "Episode:  245 Reward: 215.0 Epsilon 0.8770000000000135 mean q 2.5281608\n",
      "Episode:  246 Reward: 55.0 Epsilon 0.8765000000000136 mean q 2.5550075\n",
      "Episode:  247 Reward: 45.0 Epsilon 0.8760000000000137 mean q 2.8961964\n",
      "Episode:  248 Reward: 215.0 Epsilon 0.8755000000000137 mean q 3.4697232\n",
      "Episode:  249 Reward: 210.0 Epsilon 0.8750000000000138 mean q 2.9438813\n",
      "Episode:  250 Reward: 410.0 Epsilon 0.8745000000000138 mean q 3.0145051\n",
      "Episode:  251 Reward: 175.0 Epsilon 0.8740000000000139 mean q 4.2640333\n",
      "Episode:  252 Reward: 225.0 Epsilon 0.8735000000000139 mean q 3.7548456\n",
      "Episode:  253 Reward: 105.0 Epsilon 0.873000000000014 mean q 1.6878983\n",
      "Episode:  254 Reward: 410.0 Epsilon 0.872500000000014 mean q 2.9070685\n",
      "Episode:  255 Reward: 15.0 Epsilon 0.8720000000000141 mean q 2.4547992\n",
      "Episode:  256 Reward: 125.0 Epsilon 0.8715000000000142 mean q 3.0264685\n",
      "Episode:  257 Reward: 215.0 Epsilon 0.8710000000000142 mean q 3.6346328\n",
      "Episode:  258 Reward: 160.0 Epsilon 0.8705000000000143 mean q 4.0884566\n",
      "Episode:  259 Reward: 90.0 Epsilon 0.8700000000000143 mean q 2.7965944\n",
      "Episode:  260 Reward: 55.0 Epsilon 0.8695000000000144 mean q 3.1645355\n",
      "Episode:  261 Reward: 420.0 Epsilon 0.8690000000000144 mean q 3.794306\n",
      "Episode:  262 Reward: 210.0 Epsilon 0.8685000000000145 mean q 4.4571958\n",
      "Episode:  263 Reward: 215.0 Epsilon 0.8680000000000145 mean q 3.333477\n",
      "Episode:  264 Reward: 105.0 Epsilon 0.8675000000000146 mean q 2.7706127\n",
      "Episode:  265 Reward: 110.0 Epsilon 0.8670000000000146 mean q 3.3700433\n",
      "Episode:  266 Reward: 235.0 Epsilon 0.8665000000000147 mean q 4.6879897\n",
      "Episode:  267 Reward: 160.0 Epsilon 0.8660000000000148 mean q 3.317746\n",
      "Episode:  268 Reward: 55.0 Epsilon 0.8655000000000148 mean q 2.4381988\n",
      "Episode:  269 Reward: 100.0 Epsilon 0.8650000000000149 mean q 3.3188846\n",
      "Episode:  270 Reward: 495.0 Epsilon 0.8645000000000149 mean q 3.666728\n",
      "Episode:  271 Reward: 180.0 Epsilon 0.864000000000015 mean q 4.159013\n",
      "Episode:  272 Reward: 125.0 Epsilon 0.863500000000015 mean q 2.472363\n",
      "Episode:  273 Reward: 225.0 Epsilon 0.8630000000000151 mean q 3.97132\n",
      "Episode:  274 Reward: 75.0 Epsilon 0.8625000000000151 mean q 3.5393636\n",
      "Episode:  275 Reward: 180.0 Epsilon 0.8620000000000152 mean q 3.2151775\n",
      "Episode:  276 Reward: 320.0 Epsilon 0.8615000000000153 mean q 3.7353923\n",
      "Episode:  277 Reward: 480.0 Epsilon 0.8610000000000153 mean q 3.5553596\n",
      "Episode:  278 Reward: 60.0 Epsilon 0.8605000000000154 mean q 2.9360886\n",
      "Episode:  279 Reward: 255.0 Epsilon 0.8600000000000154 mean q 3.7222302\n",
      "Episode:  280 Reward: 155.0 Epsilon 0.8595000000000155 mean q 2.9457984\n",
      "Episode:  281 Reward: 180.0 Epsilon 0.8590000000000155 mean q 2.8935714\n",
      "Episode:  282 Reward: 135.0 Epsilon 0.8585000000000156 mean q 3.1291893\n",
      "Episode:  283 Reward: 125.0 Epsilon 0.8580000000000156 mean q 4.0293784\n",
      "Episode:  284 Reward: 45.0 Epsilon 0.8575000000000157 mean q 2.4862003\n",
      "Episode:  285 Reward: 105.0 Epsilon 0.8570000000000157 mean q 3.8120635\n",
      "Episode:  286 Reward: 180.0 Epsilon 0.8565000000000158 mean q 3.8730955\n",
      "Episode:  287 Reward: 240.0 Epsilon 0.8560000000000159 mean q 4.6125374\n",
      "Episode:  288 Reward: 65.0 Epsilon 0.8555000000000159 mean q 2.2624593\n",
      "Episode:  289 Reward: 180.0 Epsilon 0.855000000000016 mean q 3.7042794\n",
      "Episode:  290 Reward: 255.0 Epsilon 0.854500000000016 mean q 2.7830303\n",
      "Episode:  291 Reward: 180.0 Epsilon 0.8540000000000161 mean q 3.621977\n",
      "Episode:  292 Reward: 130.0 Epsilon 0.8535000000000161 mean q 3.6601028\n",
      "Episode:  293 Reward: 520.0 Epsilon 0.8530000000000162 mean q 3.9156373\n",
      "Episode:  294 Reward: 180.0 Epsilon 0.8525000000000162 mean q 4.025673\n",
      "Episode:  295 Reward: 180.0 Epsilon 0.8520000000000163 mean q 3.8271813\n",
      "Episode:  296 Reward: 135.0 Epsilon 0.8515000000000164 mean q 4.372654\n",
      "Episode:  297 Reward: 155.0 Epsilon 0.8510000000000164 mean q 3.8222811\n",
      "Episode:  298 Reward: 125.0 Epsilon 0.8505000000000165 mean q 4.1176467\n",
      "Episode:  299 Reward: 210.0 Epsilon 0.8500000000000165 mean q 3.7292755\n",
      "Episode:  300 Reward: 135.0 Epsilon 0.8495000000000166 mean q 3.2316058\n",
      "Episode:  301 Reward: 120.0 Epsilon 0.8490000000000166 mean q 3.0570035\n",
      "Episode:  302 Reward: 20.0 Epsilon 0.8485000000000167 mean q 2.694538\n",
      "Episode:  303 Reward: 155.0 Epsilon 0.8480000000000167 mean q 3.4371617\n",
      "Episode:  304 Reward: 340.0 Epsilon 0.8475000000000168 mean q 4.1885133\n",
      "Episode:  305 Reward: 120.0 Epsilon 0.8470000000000169 mean q 3.9178379\n",
      "Episode:  306 Reward: 140.0 Epsilon 0.8465000000000169 mean q 3.041238\n",
      "Episode:  307 Reward: 50.0 Epsilon 0.846000000000017 mean q 3.0148532\n",
      "Episode:  308 Reward: 140.0 Epsilon 0.845500000000017 mean q 3.3950238\n",
      "Episode:  309 Reward: 160.0 Epsilon 0.8450000000000171 mean q 3.7395477\n",
      "Episode:  310 Reward: 155.0 Epsilon 0.8445000000000171 mean q 3.016856\n",
      "Episode:  311 Reward: 65.0 Epsilon 0.8440000000000172 mean q 2.9942572\n",
      "Episode:  312 Reward: 135.0 Epsilon 0.8435000000000172 mean q 3.657485\n",
      "Episode:  313 Reward: 80.0 Epsilon 0.8430000000000173 mean q 4.0975957\n",
      "Episode:  314 Reward: 135.0 Epsilon 0.8425000000000173 mean q 2.966179\n",
      "Episode:  315 Reward: 135.0 Epsilon 0.8420000000000174 mean q 3.6197538\n",
      "Episode:  316 Reward: 120.0 Epsilon 0.8415000000000175 mean q 3.4456758\n",
      "Episode:  317 Reward: 330.0 Epsilon 0.8410000000000175 mean q 3.6553104\n",
      "Episode:  318 Reward: 150.0 Epsilon 0.8405000000000176 mean q 3.902961\n",
      "Episode:  319 Reward: 110.0 Epsilon 0.8400000000000176 mean q 2.5290318\n",
      "Episode:  320 Reward: 155.0 Epsilon 0.8395000000000177 mean q 3.0281727\n",
      "Episode:  321 Reward: 120.0 Epsilon 0.8390000000000177 mean q 4.1036716\n",
      "Episode:  322 Reward: 135.0 Epsilon 0.8385000000000178 mean q 3.0852277\n",
      "Episode:  323 Reward: 80.0 Epsilon 0.8380000000000178 mean q 4.4405317\n",
      "Episode:  324 Reward: 145.0 Epsilon 0.8375000000000179 mean q 4.1514206\n",
      "Episode:  325 Reward: 65.0 Epsilon 0.837000000000018 mean q 2.8789186\n",
      "Episode:  326 Reward: 90.0 Epsilon 0.836500000000018 mean q 3.2889452\n",
      "Episode:  327 Reward: 50.0 Epsilon 0.8360000000000181 mean q 3.356255\n",
      "Episode:  328 Reward: 155.0 Epsilon 0.8355000000000181 mean q 3.6016095\n",
      "Episode:  329 Reward: 425.0 Epsilon 0.8350000000000182 mean q 3.004151\n",
      "Episode:  330 Reward: 180.0 Epsilon 0.8345000000000182 mean q 3.935744\n",
      "Episode:  331 Reward: 90.0 Epsilon 0.8340000000000183 mean q 3.0145605\n",
      "Episode:  332 Reward: 45.0 Epsilon 0.8335000000000183 mean q 2.9636421\n",
      "Episode:  333 Reward: 35.0 Epsilon 0.8330000000000184 mean q 2.2380764\n",
      "Episode:  334 Reward: 125.0 Epsilon 0.8325000000000184 mean q 2.7809227\n",
      "Episode:  335 Reward: 180.0 Epsilon 0.8320000000000185 mean q 4.2466607\n",
      "Episode:  336 Reward: 110.0 Epsilon 0.8315000000000186 mean q 3.3330784\n",
      "Episode:  337 Reward: 50.0 Epsilon 0.8310000000000186 mean q 1.7170038\n",
      "Episode:  338 Reward: 125.0 Epsilon 0.8305000000000187 mean q 3.2079175\n",
      "Episode:  339 Reward: 80.0 Epsilon 0.8300000000000187 mean q 3.149554\n",
      "Episode:  340 Reward: 110.0 Epsilon 0.8295000000000188 mean q 2.4640515\n",
      "Episode:  341 Reward: 170.0 Epsilon 0.8290000000000188 mean q 3.8239627\n",
      "Episode:  342 Reward: 110.0 Epsilon 0.8285000000000189 mean q 3.6369987\n",
      "Episode:  343 Reward: 65.0 Epsilon 0.8280000000000189 mean q 2.6504095\n",
      "Episode:  344 Reward: 135.0 Epsilon 0.827500000000019 mean q 3.579658\n",
      "Episode:  345 Reward: 180.0 Epsilon 0.827000000000019 mean q 3.3713377\n",
      "Episode:  346 Reward: 280.0 Epsilon 0.8265000000000191 mean q 4.008044\n",
      "Episode:  347 Reward: 55.0 Epsilon 0.8260000000000192 mean q 3.3807077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  348 Reward: 160.0 Epsilon 0.8255000000000192 mean q 3.9559734\n",
      "Episode:  349 Reward: 185.0 Epsilon 0.8250000000000193 mean q 2.2612488\n",
      "Episode:  350 Reward: 160.0 Epsilon 0.8245000000000193 mean q 3.6409025\n",
      "Episode:  351 Reward: 210.0 Epsilon 0.8240000000000194 mean q 2.58619\n",
      "Episode:  352 Reward: 210.0 Epsilon 0.8235000000000194 mean q 3.15424\n",
      "Episode:  353 Reward: 165.0 Epsilon 0.8230000000000195 mean q 3.463511\n",
      "Episode:  354 Reward: 155.0 Epsilon 0.8225000000000195 mean q 3.3753948\n",
      "Episode:  355 Reward: 105.0 Epsilon 0.8220000000000196 mean q 3.0148857\n",
      "Episode:  356 Reward: 70.0 Epsilon 0.8215000000000197 mean q 3.3705156\n",
      "Episode:  357 Reward: 135.0 Epsilon 0.8210000000000197 mean q 3.0448925\n",
      "Episode:  358 Reward: 30.0 Epsilon 0.8205000000000198 mean q 2.9817352\n",
      "Episode:  359 Reward: 110.0 Epsilon 0.8200000000000198 mean q 3.6917713\n",
      "Episode:  360 Reward: 90.0 Epsilon 0.8195000000000199 mean q 3.3374076\n",
      "Episode:  361 Reward: 410.0 Epsilon 0.8190000000000199 mean q 3.3947449\n",
      "Episode:  362 Reward: 50.0 Epsilon 0.81850000000002 mean q 3.0047324\n",
      "Episode:  363 Reward: 240.0 Epsilon 0.81800000000002 mean q 3.5447333\n",
      "Episode:  364 Reward: 160.0 Epsilon 0.8175000000000201 mean q 4.296309\n",
      "Episode:  365 Reward: 65.0 Epsilon 0.8170000000000202 mean q 3.7507336\n",
      "Episode:  366 Reward: 140.0 Epsilon 0.8165000000000202 mean q 3.2733839\n",
      "Episode:  367 Reward: 30.0 Epsilon 0.8160000000000203 mean q 2.5944076\n",
      "Episode:  368 Reward: 110.0 Epsilon 0.8155000000000203 mean q 2.3675683\n",
      "Episode:  369 Reward: 265.0 Epsilon 0.8150000000000204 mean q -0.6324278\n",
      "Episode:  370 Reward: 155.0 Epsilon 0.8145000000000204 mean q 3.6024725\n",
      "Episode:  371 Reward: 60.0 Epsilon 0.8140000000000205 mean q 2.3580604\n",
      "Episode:  372 Reward: 210.0 Epsilon 0.8135000000000205 mean q 3.8276415\n",
      "Episode:  373 Reward: 120.0 Epsilon 0.8130000000000206 mean q 2.5629926\n",
      "Episode:  374 Reward: 105.0 Epsilon 0.8125000000000207 mean q 3.1543493\n",
      "Episode:  375 Reward: 100.0 Epsilon 0.8120000000000207 mean q 2.169182\n",
      "Episode:  376 Reward: 110.0 Epsilon 0.8115000000000208 mean q 3.1187882\n",
      "Episode:  377 Reward: 55.0 Epsilon 0.8110000000000208 mean q 2.1505842\n",
      "Episode:  378 Reward: 210.0 Epsilon 0.8105000000000209 mean q 3.2737844\n",
      "Episode:  379 Reward: 90.0 Epsilon 0.8100000000000209 mean q 2.6178637\n",
      "Episode:  380 Reward: 135.0 Epsilon 0.809500000000021 mean q 3.8947349\n",
      "Episode:  381 Reward: 105.0 Epsilon 0.809000000000021 mean q 3.8728836\n",
      "Episode:  382 Reward: 45.0 Epsilon 0.8085000000000211 mean q 2.8458493\n",
      "Episode:  383 Reward: 155.0 Epsilon 0.8080000000000211 mean q 3.7745833\n",
      "Episode:  384 Reward: 135.0 Epsilon 0.8075000000000212 mean q 3.0875208\n",
      "Episode:  385 Reward: 105.0 Epsilon 0.8070000000000213 mean q 3.6715853\n",
      "Episode:  386 Reward: 155.0 Epsilon 0.8065000000000213 mean q 3.921618\n",
      "Episode:  387 Reward: 105.0 Epsilon 0.8060000000000214 mean q 3.6494503\n",
      "Episode:  388 Reward: 150.0 Epsilon 0.8055000000000214 mean q 2.8469677\n",
      "Episode:  389 Reward: 135.0 Epsilon 0.8050000000000215 mean q 1.6703454\n",
      "Episode:  390 Reward: 210.0 Epsilon 0.8045000000000215 mean q 3.2974868\n",
      "Episode:  391 Reward: 110.0 Epsilon 0.8040000000000216 mean q 2.9966407\n",
      "Episode:  392 Reward: 135.0 Epsilon 0.8035000000000216 mean q 2.3619406\n",
      "Episode:  393 Reward: 55.0 Epsilon 0.8030000000000217 mean q 2.7898324\n",
      "Episode:  394 Reward: 330.0 Epsilon 0.8025000000000218 mean q 3.5723042\n",
      "Episode:  395 Reward: 90.0 Epsilon 0.8020000000000218 mean q 3.1221538\n",
      "Episode:  396 Reward: 120.0 Epsilon 0.8015000000000219 mean q 3.1666975\n",
      "Episode:  397 Reward: 310.0 Epsilon 0.8010000000000219 mean q 3.9324892\n",
      "Episode:  398 Reward: 170.0 Epsilon 0.800500000000022 mean q 2.7843287\n",
      "Episode:  399 Reward: 110.0 Epsilon 0.800000000000022 mean q 3.4669418\n",
      "Episode:  400 Reward: 75.0 Epsilon 0.7995000000000221 mean q 3.624205\n",
      "Episode:  401 Reward: 155.0 Epsilon 0.7990000000000221 mean q 3.4493778\n",
      "Episode:  402 Reward: 120.0 Epsilon 0.7985000000000222 mean q 2.4174268\n",
      "Episode:  403 Reward: 120.0 Epsilon 0.7980000000000222 mean q 3.9451022\n",
      "Episode:  404 Reward: 105.0 Epsilon 0.7975000000000223 mean q 3.6586277\n",
      "Episode:  405 Reward: 230.0 Epsilon 0.7970000000000224 mean q 4.7259593\n",
      "Episode:  406 Reward: 175.0 Epsilon 0.7965000000000224 mean q 4.048257\n",
      "Episode:  407 Reward: 110.0 Epsilon 0.7960000000000225 mean q 2.9862409\n",
      "Episode:  408 Reward: 225.0 Epsilon 0.7955000000000225 mean q 3.744618\n",
      "Episode:  409 Reward: 35.0 Epsilon 0.7950000000000226 mean q 1.1651276\n",
      "Episode:  410 Reward: 110.0 Epsilon 0.7945000000000226 mean q 1.7351784\n",
      "Episode:  411 Reward: 45.0 Epsilon 0.7940000000000227 mean q 2.530815\n",
      "Episode:  412 Reward: 50.0 Epsilon 0.7935000000000227 mean q 3.2455902\n",
      "Episode:  413 Reward: 45.0 Epsilon 0.7930000000000228 mean q 3.4930036\n",
      "Episode:  414 Reward: 120.0 Epsilon 0.7925000000000229 mean q 3.793956\n",
      "Episode:  415 Reward: 260.0 Epsilon 0.7920000000000229 mean q 3.9188707\n",
      "Episode:  416 Reward: 210.0 Epsilon 0.791500000000023 mean q 2.578125\n",
      "Episode:  417 Reward: 250.0 Epsilon 0.791000000000023 mean q 3.1295516\n",
      "Episode:  418 Reward: 160.0 Epsilon 0.7905000000000231 mean q 3.8646293\n",
      "Episode:  419 Reward: 110.0 Epsilon 0.7900000000000231 mean q 2.37354\n",
      "Episode:  420 Reward: 395.0 Epsilon 0.7895000000000232 mean q 4.11533\n",
      "Episode:  421 Reward: 240.0 Epsilon 0.7890000000000232 mean q 3.6582048\n",
      "Episode:  422 Reward: 180.0 Epsilon 0.7885000000000233 mean q 3.5289226\n",
      "Episode:  423 Reward: 80.0 Epsilon 0.7880000000000233 mean q 3.776867\n",
      "Episode:  424 Reward: 210.0 Epsilon 0.7875000000000234 mean q 4.0929675\n",
      "Episode:  425 Reward: 130.0 Epsilon 0.7870000000000235 mean q 3.846594\n",
      "Episode:  426 Reward: 210.0 Epsilon 0.7865000000000235 mean q 3.7085779\n",
      "Episode:  427 Reward: 210.0 Epsilon 0.7860000000000236 mean q 2.6748066\n",
      "Episode:  428 Reward: 165.0 Epsilon 0.7855000000000236 mean q 4.309079\n",
      "Episode:  429 Reward: 145.0 Epsilon 0.7850000000000237 mean q 3.6506822\n",
      "Episode:  430 Reward: 50.0 Epsilon 0.7845000000000237 mean q 2.9611616\n",
      "Episode:  431 Reward: 30.0 Epsilon 0.7840000000000238 mean q 1.7732224\n",
      "Episode:  432 Reward: 105.0 Epsilon 0.7835000000000238 mean q 3.868415\n",
      "Episode:  433 Reward: 105.0 Epsilon 0.7830000000000239 mean q 2.9306798\n",
      "Episode:  434 Reward: 230.0 Epsilon 0.782500000000024 mean q 4.3305326\n",
      "Episode:  435 Reward: 130.0 Epsilon 0.782000000000024 mean q 3.000175\n",
      "Episode:  436 Reward: 425.0 Epsilon 0.7815000000000241 mean q 3.589871\n",
      "Episode:  437 Reward: 435.0 Epsilon 0.7810000000000241 mean q 3.8664622\n",
      "Episode:  438 Reward: 130.0 Epsilon 0.7805000000000242 mean q 3.6975074\n",
      "Episode:  439 Reward: 190.0 Epsilon 0.7800000000000242 mean q 3.2864013\n",
      "Episode:  440 Reward: 35.0 Epsilon 0.7795000000000243 mean q 3.1972206\n",
      "Episode:  441 Reward: 125.0 Epsilon 0.7790000000000243 mean q 3.1147966\n",
      "Episode:  442 Reward: 55.0 Epsilon 0.7785000000000244 mean q 2.6492796\n",
      "Episode:  443 Reward: 210.0 Epsilon 0.7780000000000244 mean q 3.8734725\n",
      "Episode:  444 Reward: 210.0 Epsilon 0.7775000000000245 mean q 3.7457178\n",
      "Episode:  445 Reward: 105.0 Epsilon 0.7770000000000246 mean q 4.1238413\n",
      "Episode:  446 Reward: 180.0 Epsilon 0.7765000000000246 mean q 2.5587611\n",
      "Episode:  447 Reward: 120.0 Epsilon 0.7760000000000247 mean q 3.2649255\n",
      "Episode:  448 Reward: 155.0 Epsilon 0.7755000000000247 mean q 2.3674428\n",
      "Episode:  449 Reward: 185.0 Epsilon 0.7750000000000248 mean q 3.3747094\n",
      "Episode:  450 Reward: 120.0 Epsilon 0.7745000000000248 mean q 2.7120073\n",
      "Episode:  451 Reward: 475.0 Epsilon 0.7740000000000249 mean q 4.4978538\n",
      "Episode:  452 Reward: 255.0 Epsilon 0.773500000000025 mean q 3.345651\n",
      "Episode:  453 Reward: 110.0 Epsilon 0.773000000000025 mean q 2.4445972\n",
      "Episode:  454 Reward: 140.0 Epsilon 0.7725000000000251 mean q 4.1245503\n",
      "Episode:  455 Reward: 210.0 Epsilon 0.7720000000000251 mean q 3.689111\n",
      "Episode:  456 Reward: 140.0 Epsilon 0.7715000000000252 mean q 2.797843\n",
      "Episode:  457 Reward: 105.0 Epsilon 0.7710000000000252 mean q 3.9695332\n",
      "Episode:  458 Reward: 215.0 Epsilon 0.7705000000000253 mean q 4.508191\n",
      "Episode:  459 Reward: 455.0 Epsilon 0.7700000000000253 mean q 3.8166416\n",
      "Episode:  460 Reward: 50.0 Epsilon 0.7695000000000254 mean q 2.6292489\n",
      "Episode:  461 Reward: 50.0 Epsilon 0.7690000000000254 mean q 3.4735453\n",
      "Episode:  462 Reward: 110.0 Epsilon 0.7685000000000255 mean q 2.9380941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  463 Reward: 155.0 Epsilon 0.7680000000000256 mean q 3.2587206\n",
      "Episode:  464 Reward: 90.0 Epsilon 0.7675000000000256 mean q 4.3621726\n",
      "Episode:  465 Reward: 125.0 Epsilon 0.7670000000000257 mean q 3.1367328\n",
      "Episode:  466 Reward: 185.0 Epsilon 0.7665000000000257 mean q 3.6708233\n",
      "Episode:  467 Reward: 110.0 Epsilon 0.7660000000000258 mean q 2.48805\n",
      "Episode:  468 Reward: 50.0 Epsilon 0.7655000000000258 mean q 2.2021458\n",
      "Episode:  469 Reward: 180.0 Epsilon 0.7650000000000259 mean q 3.497818\n",
      "Episode:  470 Reward: 70.0 Epsilon 0.7645000000000259 mean q 2.4310052\n",
      "Episode:  471 Reward: 135.0 Epsilon 0.764000000000026 mean q 3.748805\n",
      "Episode:  472 Reward: 155.0 Epsilon 0.763500000000026 mean q 2.9159439\n",
      "Episode:  473 Reward: 200.0 Epsilon 0.7630000000000261 mean q 3.5930617\n",
      "Episode:  474 Reward: 155.0 Epsilon 0.7625000000000262 mean q 4.217102\n",
      "Episode:  475 Reward: 240.0 Epsilon 0.7620000000000262 mean q 3.2368746\n",
      "Episode:  476 Reward: 180.0 Epsilon 0.7615000000000263 mean q 3.859058\n",
      "Episode:  477 Reward: 135.0 Epsilon 0.7610000000000263 mean q 3.3543687\n",
      "Episode:  478 Reward: 45.0 Epsilon 0.7605000000000264 mean q 3.5896797\n",
      "Episode:  479 Reward: 45.0 Epsilon 0.7600000000000264 mean q 3.7696226\n",
      "Episode:  480 Reward: 80.0 Epsilon 0.7595000000000265 mean q 2.0082023\n",
      "Episode:  481 Reward: 125.0 Epsilon 0.7590000000000265 mean q 3.7941165\n",
      "Episode:  482 Reward: 95.0 Epsilon 0.7585000000000266 mean q 2.9759064\n",
      "Episode:  483 Reward: 260.0 Epsilon 0.7580000000000267 mean q 4.0413947\n",
      "Episode:  484 Reward: 155.0 Epsilon 0.7575000000000267 mean q 3.4918206\n",
      "Episode:  485 Reward: 35.0 Epsilon 0.7570000000000268 mean q 2.024329\n",
      "Episode:  486 Reward: 140.0 Epsilon 0.7565000000000268 mean q 4.5020604\n",
      "Episode:  487 Reward: 210.0 Epsilon 0.7560000000000269 mean q 3.4850142\n",
      "Episode:  488 Reward: 410.0 Epsilon 0.7555000000000269 mean q 3.5825448\n",
      "Episode:  489 Reward: 45.0 Epsilon 0.755000000000027 mean q 3.0864737\n",
      "Episode:  490 Reward: 135.0 Epsilon 0.754500000000027 mean q 3.0965393\n",
      "Episode:  491 Reward: 35.0 Epsilon 0.7540000000000271 mean q 1.823208\n",
      "Episode:  492 Reward: 180.0 Epsilon 0.7535000000000271 mean q 3.239646\n",
      "Episode:  493 Reward: 205.0 Epsilon 0.7530000000000272 mean q 4.3652706\n",
      "Episode:  494 Reward: 380.0 Epsilon 0.7525000000000273 mean q 4.5099983\n",
      "Episode:  495 Reward: 135.0 Epsilon 0.7520000000000273 mean q 2.0090084\n",
      "Episode:  496 Reward: 155.0 Epsilon 0.7515000000000274 mean q 3.1019542\n",
      "Episode:  497 Reward: 395.0 Epsilon 0.7510000000000274 mean q 2.8708565\n",
      "Episode:  498 Reward: 420.0 Epsilon 0.7505000000000275 mean q 2.9747741\n",
      "Episode:  499 Reward: 125.0 Epsilon 0.7500000000000275 mean q 2.9702764\n",
      "Episode:  500 Reward: 160.0 Epsilon 0.7495000000000276 mean q 3.7961383\n",
      "Episode:  501 Reward: 120.0 Epsilon 0.7490000000000276 mean q 3.4932632\n",
      "Episode:  502 Reward: 120.0 Epsilon 0.7485000000000277 mean q 3.3618917\n",
      "Episode:  503 Reward: 130.0 Epsilon 0.7480000000000278 mean q 3.5442445\n",
      "Episode:  504 Reward: 270.0 Epsilon 0.7475000000000278 mean q 4.312204\n",
      "Episode:  505 Reward: 105.0 Epsilon 0.7470000000000279 mean q 3.4014175\n",
      "Episode:  506 Reward: 180.0 Epsilon 0.7465000000000279 mean q 4.238022\n",
      "Episode:  507 Reward: 45.0 Epsilon 0.746000000000028 mean q 2.6163304\n",
      "Episode:  508 Reward: 430.0 Epsilon 0.745500000000028 mean q 3.3396728\n",
      "Episode:  509 Reward: 180.0 Epsilon 0.7450000000000281 mean q 2.7542567\n",
      "Episode:  510 Reward: 95.0 Epsilon 0.7445000000000281 mean q 3.9368734\n",
      "Episode:  511 Reward: 345.0 Epsilon 0.7440000000000282 mean q 3.5328817\n",
      "Episode:  512 Reward: 185.0 Epsilon 0.7435000000000282 mean q 3.1353009\n",
      "Episode:  513 Reward: 20.0 Epsilon 0.7430000000000283 mean q 3.0806735\n",
      "Episode:  514 Reward: 135.0 Epsilon 0.7425000000000284 mean q 3.146065\n",
      "Episode:  515 Reward: 130.0 Epsilon 0.7420000000000284 mean q 4.446976\n",
      "Episode:  516 Reward: 245.0 Epsilon 0.7415000000000285 mean q 5.065502\n",
      "Episode:  517 Reward: 225.0 Epsilon 0.7410000000000285 mean q 2.9573915\n",
      "Episode:  518 Reward: 120.0 Epsilon 0.7405000000000286 mean q 2.8794625\n",
      "Episode:  519 Reward: 45.0 Epsilon 0.7400000000000286 mean q 3.7633815\n",
      "Episode:  520 Reward: 135.0 Epsilon 0.7395000000000287 mean q 3.1193013\n",
      "Episode:  521 Reward: 180.0 Epsilon 0.7390000000000287 mean q 3.382892\n",
      "Episode:  522 Reward: 180.0 Epsilon 0.7385000000000288 mean q 3.9384868\n",
      "Episode:  523 Reward: 195.0 Epsilon 0.7380000000000289 mean q 4.1290164\n",
      "Episode:  524 Reward: 85.0 Epsilon 0.7375000000000289 mean q 3.4335902\n",
      "Episode:  525 Reward: 410.0 Epsilon 0.737000000000029 mean q 3.5881138\n",
      "Episode:  526 Reward: 255.0 Epsilon 0.736500000000029 mean q 3.9087923\n",
      "Episode:  527 Reward: 35.0 Epsilon 0.7360000000000291 mean q 2.7079241\n",
      "Episode:  528 Reward: 260.0 Epsilon 0.7355000000000291 mean q 3.908201\n",
      "Episode:  529 Reward: 110.0 Epsilon 0.7350000000000292 mean q 2.821419\n",
      "Episode:  530 Reward: 140.0 Epsilon 0.7345000000000292 mean q 3.8449259\n",
      "Episode:  531 Reward: 110.0 Epsilon 0.7340000000000293 mean q 2.5988386\n",
      "Episode:  532 Reward: 170.0 Epsilon 0.7335000000000294 mean q 4.142791\n",
      "Episode:  533 Reward: 30.0 Epsilon 0.7330000000000294 mean q 3.135769\n",
      "Episode:  534 Reward: 255.0 Epsilon 0.7325000000000295 mean q 3.70881\n",
      "Episode:  535 Reward: 160.0 Epsilon 0.7320000000000295 mean q 2.6792748\n",
      "Episode:  536 Reward: 105.0 Epsilon 0.7315000000000296 mean q 4.028872\n",
      "Episode:  537 Reward: 205.0 Epsilon 0.7310000000000296 mean q 3.794463\n",
      "Episode:  538 Reward: 120.0 Epsilon 0.7305000000000297 mean q 2.9881864\n",
      "Episode:  539 Reward: 180.0 Epsilon 0.7300000000000297 mean q 3.5451796\n",
      "Episode:  540 Reward: 210.0 Epsilon 0.7295000000000298 mean q 3.802186\n",
      "Episode:  541 Reward: 210.0 Epsilon 0.7290000000000298 mean q 2.8960319\n",
      "Episode:  542 Reward: 80.0 Epsilon 0.7285000000000299 mean q 3.2811425\n",
      "Episode:  543 Reward: 35.0 Epsilon 0.72800000000003 mean q 2.9592466\n",
      "Episode:  544 Reward: 65.0 Epsilon 0.72750000000003 mean q 1.7519907\n",
      "Episode:  545 Reward: 320.0 Epsilon 0.7270000000000301 mean q 3.809334\n",
      "Episode:  546 Reward: 185.0 Epsilon 0.7265000000000301 mean q 3.7929008\n",
      "Episode:  547 Reward: 125.0 Epsilon 0.7260000000000302 mean q 2.8532956\n",
      "Episode:  548 Reward: 105.0 Epsilon 0.7255000000000302 mean q 2.503585\n",
      "Episode:  549 Reward: 180.0 Epsilon 0.7250000000000303 mean q 3.908586\n",
      "Episode:  550 Reward: 125.0 Epsilon 0.7245000000000303 mean q 3.0506496\n",
      "Episode:  551 Reward: 180.0 Epsilon 0.7240000000000304 mean q 3.7932963\n",
      "Episode:  552 Reward: 45.0 Epsilon 0.7235000000000305 mean q 2.3212392\n",
      "Episode:  553 Reward: 115.0 Epsilon 0.7230000000000305 mean q 3.6746743\n",
      "Episode:  554 Reward: 150.0 Epsilon 0.7225000000000306 mean q 4.309725\n",
      "Episode:  555 Reward: 45.0 Epsilon 0.7220000000000306 mean q 3.7814016\n",
      "Episode:  556 Reward: 125.0 Epsilon 0.7215000000000307 mean q 3.1342652\n",
      "Episode:  557 Reward: 30.0 Epsilon 0.7210000000000307 mean q 3.0328522\n",
      "Episode:  558 Reward: 425.0 Epsilon 0.7205000000000308 mean q 4.234529\n",
      "Episode:  559 Reward: 145.0 Epsilon 0.7200000000000308 mean q 4.0077596\n",
      "Episode:  560 Reward: 65.0 Epsilon 0.7195000000000309 mean q 2.5338023\n",
      "Episode:  561 Reward: 125.0 Epsilon 0.719000000000031 mean q 3.5612447\n",
      "Episode:  562 Reward: 285.0 Epsilon 0.718500000000031 mean q 3.9455884\n",
      "Episode:  563 Reward: 75.0 Epsilon 0.7180000000000311 mean q 3.3589277\n",
      "Episode:  564 Reward: 20.0 Epsilon 0.7175000000000311 mean q 2.4394586\n",
      "Episode:  565 Reward: 125.0 Epsilon 0.7170000000000312 mean q 3.8171952\n",
      "Episode:  566 Reward: 185.0 Epsilon 0.7165000000000312 mean q 3.3740106\n",
      "Episode:  567 Reward: 30.0 Epsilon 0.7160000000000313 mean q 3.066068\n",
      "Episode:  568 Reward: 155.0 Epsilon 0.7155000000000313 mean q 4.3188396\n",
      "Episode:  569 Reward: 135.0 Epsilon 0.7150000000000314 mean q 2.3680813\n",
      "Episode:  570 Reward: 165.0 Epsilon 0.7145000000000314 mean q 3.891114\n",
      "Episode:  571 Reward: 85.0 Epsilon 0.7140000000000315 mean q 3.0866857\n",
      "Episode:  572 Reward: 75.0 Epsilon 0.7135000000000316 mean q 3.222657\n",
      "Episode:  573 Reward: 120.0 Epsilon 0.7130000000000316 mean q 4.051406\n",
      "Episode:  574 Reward: 80.0 Epsilon 0.7125000000000317 mean q 3.243001\n",
      "Episode:  575 Reward: 145.0 Epsilon 0.7120000000000317 mean q 3.5283856\n",
      "Episode:  576 Reward: 100.0 Epsilon 0.7115000000000318 mean q 3.2932374\n",
      "Episode:  577 Reward: 210.0 Epsilon 0.7110000000000318 mean q 4.3284593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  578 Reward: 105.0 Epsilon 0.7105000000000319 mean q 2.8126466\n",
      "Episode:  579 Reward: 115.0 Epsilon 0.7100000000000319 mean q 2.6591527\n",
      "Episode:  580 Reward: 80.0 Epsilon 0.709500000000032 mean q 3.116161\n",
      "Episode:  581 Reward: 45.0 Epsilon 0.709000000000032 mean q 3.2482011\n",
      "Episode:  582 Reward: 180.0 Epsilon 0.7085000000000321 mean q 2.4185548\n",
      "Episode:  583 Reward: 215.0 Epsilon 0.7080000000000322 mean q 3.5559301\n",
      "Episode:  584 Reward: 30.0 Epsilon 0.7075000000000322 mean q 2.0250971\n",
      "Episode:  585 Reward: 335.0 Epsilon 0.7070000000000323 mean q 3.6653533\n",
      "Episode:  586 Reward: 160.0 Epsilon 0.7065000000000323 mean q 3.8574061\n",
      "Episode:  587 Reward: 415.0 Epsilon 0.7060000000000324 mean q 4.310044\n",
      "Episode:  588 Reward: 125.0 Epsilon 0.7055000000000324 mean q 3.2323015\n",
      "Episode:  589 Reward: 125.0 Epsilon 0.7050000000000325 mean q 2.723282\n",
      "Episode:  590 Reward: 15.0 Epsilon 0.7045000000000325 mean q 2.293179\n",
      "Episode:  591 Reward: 210.0 Epsilon 0.7040000000000326 mean q 3.8272307\n",
      "Episode:  592 Reward: 180.0 Epsilon 0.7035000000000327 mean q 2.987717\n",
      "Episode:  593 Reward: 210.0 Epsilon 0.7030000000000327 mean q 3.1259403\n",
      "Episode:  594 Reward: 60.0 Epsilon 0.7025000000000328 mean q 2.6848917\n",
      "Episode:  595 Reward: 185.0 Epsilon 0.7020000000000328 mean q 4.4306107\n",
      "Episode:  596 Reward: 155.0 Epsilon 0.7015000000000329 mean q 2.2964022\n",
      "Episode:  597 Reward: 185.0 Epsilon 0.7010000000000329 mean q 3.7760656\n",
      "Episode:  598 Reward: 50.0 Epsilon 0.700500000000033 mean q 2.5896\n",
      "Episode:  599 Reward: 155.0 Epsilon 0.700000000000033 mean q 2.6759262\n",
      "Episode:  600 Reward: 235.0 Epsilon 0.6995000000000331 mean q 3.2936811\n",
      "Episode:  601 Reward: 150.0 Epsilon 0.6990000000000332 mean q 4.6255713\n",
      "Episode:  602 Reward: 275.0 Epsilon 0.6985000000000332 mean q 4.227276\n",
      "Episode:  603 Reward: 100.0 Epsilon 0.6980000000000333 mean q 2.8557837\n",
      "Episode:  604 Reward: 130.0 Epsilon 0.6975000000000333 mean q 3.5103273\n",
      "Episode:  605 Reward: 75.0 Epsilon 0.6970000000000334 mean q 2.7166908\n",
      "Episode:  606 Reward: 80.0 Epsilon 0.6965000000000334 mean q 2.9567497\n",
      "Episode:  607 Reward: 240.0 Epsilon 0.6960000000000335 mean q 3.7993805\n",
      "Episode:  608 Reward: 55.0 Epsilon 0.6955000000000335 mean q 2.972613\n",
      "Episode:  609 Reward: 210.0 Epsilon 0.6950000000000336 mean q 2.44017\n",
      "Episode:  610 Reward: 260.0 Epsilon 0.6945000000000336 mean q 3.5240996\n",
      "Episode:  611 Reward: 100.0 Epsilon 0.6940000000000337 mean q 2.8466225\n",
      "Episode:  612 Reward: 85.0 Epsilon 0.6935000000000338 mean q 3.3244615\n",
      "Episode:  613 Reward: 185.0 Epsilon 0.6930000000000338 mean q 2.5604813\n",
      "Episode:  614 Reward: 85.0 Epsilon 0.6925000000000339 mean q 2.8763719\n",
      "Episode:  615 Reward: 180.0 Epsilon 0.6920000000000339 mean q 3.9940817\n",
      "Episode:  616 Reward: 160.0 Epsilon 0.691500000000034 mean q 4.0942264\n",
      "Episode:  617 Reward: 105.0 Epsilon 0.691000000000034 mean q 1.9682943\n",
      "Episode:  618 Reward: 50.0 Epsilon 0.6905000000000341 mean q 2.441043\n",
      "Episode:  619 Reward: 95.0 Epsilon 0.6900000000000341 mean q 3.8697002\n",
      "Episode:  620 Reward: 105.0 Epsilon 0.6895000000000342 mean q 3.3147185\n",
      "Episode:  621 Reward: 210.0 Epsilon 0.6890000000000343 mean q 2.7203157\n",
      "Episode:  622 Reward: 115.0 Epsilon 0.6885000000000343 mean q 3.4984128\n",
      "Episode:  623 Reward: 15.0 Epsilon 0.6880000000000344 mean q 2.7517083\n",
      "Episode:  624 Reward: 315.0 Epsilon 0.6875000000000344 mean q 3.0617714\n",
      "Episode:  625 Reward: 440.0 Epsilon 0.6870000000000345 mean q 2.936206\n",
      "Episode:  626 Reward: 125.0 Epsilon 0.6865000000000345 mean q 2.7950644\n",
      "Episode:  627 Reward: 215.0 Epsilon 0.6860000000000346 mean q 4.5186086\n",
      "Episode:  628 Reward: 135.0 Epsilon 0.6855000000000346 mean q 3.537391\n",
      "Episode:  629 Reward: 170.0 Epsilon 0.6850000000000347 mean q 4.761512\n",
      "Episode:  630 Reward: 300.0 Epsilon 0.6845000000000347 mean q 4.122813\n",
      "Episode:  631 Reward: 35.0 Epsilon 0.6840000000000348 mean q 2.4708095\n",
      "Episode:  632 Reward: 130.0 Epsilon 0.6835000000000349 mean q 3.308113\n",
      "Episode:  633 Reward: 105.0 Epsilon 0.6830000000000349 mean q 2.3202267\n",
      "Episode:  634 Reward: 390.0 Epsilon 0.682500000000035 mean q 3.7405274\n",
      "Episode:  635 Reward: 50.0 Epsilon 0.682000000000035 mean q 3.3608904\n",
      "Episode:  636 Reward: 185.0 Epsilon 0.6815000000000351 mean q 3.0334344\n",
      "Episode:  637 Reward: 410.0 Epsilon 0.6810000000000351 mean q 3.9322565\n",
      "Episode:  638 Reward: 60.0 Epsilon 0.6805000000000352 mean q 3.350423\n",
      "Episode:  639 Reward: 100.0 Epsilon 0.6800000000000352 mean q 2.7759428\n",
      "Episode:  640 Reward: 90.0 Epsilon 0.6795000000000353 mean q 4.0923553\n",
      "Episode:  641 Reward: 265.0 Epsilon 0.6790000000000354 mean q 4.198972\n",
      "Episode:  642 Reward: 145.0 Epsilon 0.6785000000000354 mean q 3.103734\n",
      "Episode:  643 Reward: 50.0 Epsilon 0.6780000000000355 mean q 2.0983384\n",
      "Episode:  644 Reward: 180.0 Epsilon 0.6775000000000355 mean q 3.2571273\n",
      "Episode:  645 Reward: 365.0 Epsilon 0.6770000000000356 mean q 3.7103806\n",
      "Episode:  646 Reward: 385.0 Epsilon 0.6765000000000356 mean q 3.3308065\n",
      "Episode:  647 Reward: 80.0 Epsilon 0.6760000000000357 mean q 3.0529306\n",
      "Episode:  648 Reward: 150.0 Epsilon 0.6755000000000357 mean q 3.0797725\n",
      "Episode:  649 Reward: 120.0 Epsilon 0.6750000000000358 mean q 4.261233\n",
      "Episode:  650 Reward: 30.0 Epsilon 0.6745000000000358 mean q 2.6986146\n",
      "Episode:  651 Reward: 210.0 Epsilon 0.6740000000000359 mean q 3.0532744\n",
      "Episode:  652 Reward: 90.0 Epsilon 0.673500000000036 mean q 1.6484574\n",
      "Episode:  653 Reward: 180.0 Epsilon 0.673000000000036 mean q 3.713333\n",
      "Episode:  654 Reward: 210.0 Epsilon 0.6725000000000361 mean q 3.445407\n",
      "Episode:  655 Reward: 170.0 Epsilon 0.6720000000000361 mean q 3.625582\n",
      "Episode:  656 Reward: 275.0 Epsilon 0.6715000000000362 mean q 3.790893\n",
      "Episode:  657 Reward: 120.0 Epsilon 0.6710000000000362 mean q 2.5792422\n",
      "Episode:  658 Reward: 55.0 Epsilon 0.6705000000000363 mean q 3.4031549\n",
      "Episode:  659 Reward: 70.0 Epsilon 0.6700000000000363 mean q 3.6578774\n",
      "Episode:  660 Reward: 230.0 Epsilon 0.6695000000000364 mean q 4.139609\n",
      "Episode:  661 Reward: 60.0 Epsilon 0.6690000000000365 mean q 2.408661\n",
      "Episode:  662 Reward: 135.0 Epsilon 0.6685000000000365 mean q 3.2902083\n",
      "Episode:  663 Reward: 80.0 Epsilon 0.6680000000000366 mean q 3.797459\n",
      "Episode:  664 Reward: 150.0 Epsilon 0.6675000000000366 mean q 3.3101313\n",
      "Episode:  665 Reward: 155.0 Epsilon 0.6670000000000367 mean q 4.3849664\n",
      "Episode:  666 Reward: 310.0 Epsilon 0.6665000000000367 mean q 3.2942758\n",
      "Episode:  667 Reward: 45.0 Epsilon 0.6660000000000368 mean q 3.0276043\n",
      "Episode:  668 Reward: 110.0 Epsilon 0.6655000000000368 mean q 3.0682204\n",
      "Episode:  669 Reward: 85.0 Epsilon 0.6650000000000369 mean q 4.206317\n",
      "Episode:  670 Reward: 135.0 Epsilon 0.664500000000037 mean q 2.147975\n",
      "Episode:  671 Reward: 125.0 Epsilon 0.664000000000037 mean q 3.7700186\n",
      "Episode:  672 Reward: 210.0 Epsilon 0.6635000000000371 mean q 3.0837758\n",
      "Episode:  673 Reward: 135.0 Epsilon 0.6630000000000371 mean q 2.812218\n",
      "Episode:  674 Reward: 20.0 Epsilon 0.6625000000000372 mean q 2.8651216\n",
      "Episode:  675 Reward: 280.0 Epsilon 0.6620000000000372 mean q 4.0478106\n",
      "Episode:  676 Reward: 45.0 Epsilon 0.6615000000000373 mean q 3.492289\n",
      "Episode:  677 Reward: 460.0 Epsilon 0.6610000000000373 mean q 3.3560028\n",
      "Episode:  678 Reward: 50.0 Epsilon 0.6605000000000374 mean q 2.4086447\n",
      "Episode:  679 Reward: 135.0 Epsilon 0.6600000000000374 mean q 3.9049528\n",
      "Episode:  680 Reward: 210.0 Epsilon 0.6595000000000375 mean q 3.0062873\n",
      "Episode:  681 Reward: 105.0 Epsilon 0.6590000000000376 mean q 4.6043878\n",
      "Episode:  682 Reward: 30.0 Epsilon 0.6585000000000376 mean q 3.6776865\n",
      "Episode:  683 Reward: 110.0 Epsilon 0.6580000000000377 mean q 3.1681833\n",
      "Episode:  684 Reward: 170.0 Epsilon 0.6575000000000377 mean q 2.8586268\n",
      "Episode:  685 Reward: 145.0 Epsilon 0.6570000000000378 mean q 3.0501573\n",
      "Episode:  686 Reward: 55.0 Epsilon 0.6565000000000378 mean q 2.8982093\n",
      "Episode:  687 Reward: 140.0 Epsilon 0.6560000000000379 mean q 3.5042014\n",
      "Episode:  688 Reward: 55.0 Epsilon 0.6555000000000379 mean q 2.6191845\n",
      "Episode:  689 Reward: 285.0 Epsilon 0.655000000000038 mean q 3.2797232\n",
      "Episode:  690 Reward: 90.0 Epsilon 0.654500000000038 mean q 2.781297\n",
      "Episode:  691 Reward: 210.0 Epsilon 0.6540000000000381 mean q 2.8072324\n",
      "Episode:  692 Reward: 185.0 Epsilon 0.6535000000000382 mean q 3.2581358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  693 Reward: 135.0 Epsilon 0.6530000000000382 mean q 3.3731885\n",
      "Episode:  694 Reward: 155.0 Epsilon 0.6525000000000383 mean q 4.377128\n",
      "Episode:  695 Reward: 210.0 Epsilon 0.6520000000000383 mean q 4.111068\n",
      "Episode:  696 Reward: 125.0 Epsilon 0.6515000000000384 mean q 3.848222\n",
      "Episode:  697 Reward: 185.0 Epsilon 0.6510000000000384 mean q 3.7114022\n",
      "Episode:  698 Reward: 90.0 Epsilon 0.6505000000000385 mean q 2.6845212\n",
      "Episode:  699 Reward: 110.0 Epsilon 0.6500000000000385 mean q 2.5768666\n",
      "Episode:  700 Reward: 20.0 Epsilon 0.6495000000000386 mean q 2.3819883\n",
      "Episode:  701 Reward: 100.0 Epsilon 0.6490000000000387 mean q 3.290349\n",
      "Episode:  702 Reward: 135.0 Epsilon 0.6485000000000387 mean q 3.757516\n",
      "Episode:  703 Reward: 330.0 Epsilon 0.6480000000000388 mean q 3.9556198\n",
      "Episode:  704 Reward: 215.0 Epsilon 0.6475000000000388 mean q 3.8458486\n",
      "Episode:  705 Reward: 105.0 Epsilon 0.6470000000000389 mean q 2.9117167\n",
      "Episode:  706 Reward: 90.0 Epsilon 0.6465000000000389 mean q 2.3599963\n",
      "Episode:  707 Reward: 265.0 Epsilon 0.646000000000039 mean q 3.4183836\n",
      "Episode:  708 Reward: 305.0 Epsilon 0.645500000000039 mean q 3.1875799\n",
      "Episode:  709 Reward: 115.0 Epsilon 0.6450000000000391 mean q 3.2446523\n",
      "Episode:  710 Reward: 140.0 Epsilon 0.6445000000000392 mean q 3.7764902\n",
      "Episode:  711 Reward: 210.0 Epsilon 0.6440000000000392 mean q 3.7423563\n",
      "Episode:  712 Reward: 565.0 Epsilon 0.6435000000000393 mean q 2.943324\n",
      "Episode:  713 Reward: 180.0 Epsilon 0.6430000000000393 mean q 3.8125238\n",
      "Episode:  714 Reward: 45.0 Epsilon 0.6425000000000394 mean q 3.0966392\n",
      "Episode:  715 Reward: 155.0 Epsilon 0.6420000000000394 mean q 3.0420728\n",
      "Episode:  716 Reward: 125.0 Epsilon 0.6415000000000395 mean q 3.9052525\n",
      "Episode:  717 Reward: 140.0 Epsilon 0.6410000000000395 mean q 3.3146806\n",
      "Episode:  718 Reward: 80.0 Epsilon 0.6405000000000396 mean q 3.1245778\n",
      "Episode:  719 Reward: 240.0 Epsilon 0.6400000000000396 mean q 4.1052313\n",
      "Episode:  720 Reward: 155.0 Epsilon 0.6395000000000397 mean q 3.9597762\n",
      "Episode:  721 Reward: 120.0 Epsilon 0.6390000000000398 mean q 4.192091\n",
      "Episode:  722 Reward: 155.0 Epsilon 0.6385000000000398 mean q 3.3911312\n",
      "Episode:  723 Reward: 85.0 Epsilon 0.6380000000000399 mean q 3.3767633\n",
      "Episode:  724 Reward: 165.0 Epsilon 0.6375000000000399 mean q 3.7031708\n",
      "Episode:  725 Reward: 345.0 Epsilon 0.63700000000004 mean q 3.4574742\n",
      "Episode:  726 Reward: 370.0 Epsilon 0.63650000000004 mean q 4.514762\n",
      "Episode:  727 Reward: 225.0 Epsilon 0.6360000000000401 mean q 2.820579\n",
      "Episode:  728 Reward: 65.0 Epsilon 0.6355000000000401 mean q 3.745436\n",
      "Episode:  729 Reward: 150.0 Epsilon 0.6350000000000402 mean q 3.6552987\n",
      "Episode:  730 Reward: 20.0 Epsilon 0.6345000000000403 mean q 3.1305096\n",
      "Episode:  731 Reward: 315.0 Epsilon 0.6340000000000403 mean q 2.7700639\n",
      "Episode:  732 Reward: 375.0 Epsilon 0.6335000000000404 mean q 3.5565956\n",
      "Episode:  733 Reward: 45.0 Epsilon 0.6330000000000404 mean q 2.5807636\n",
      "Episode:  734 Reward: 225.0 Epsilon 0.6325000000000405 mean q 3.8040092\n",
      "Episode:  735 Reward: 180.0 Epsilon 0.6320000000000405 mean q 3.7803342\n",
      "Episode:  736 Reward: 395.0 Epsilon 0.6315000000000406 mean q 1.0819064\n",
      "Episode:  737 Reward: 110.0 Epsilon 0.6310000000000406 mean q 3.8059282\n",
      "Episode:  738 Reward: 50.0 Epsilon 0.6305000000000407 mean q 2.0790734\n",
      "Episode:  739 Reward: 195.0 Epsilon 0.6300000000000407 mean q 3.859103\n",
      "Episode:  740 Reward: 185.0 Epsilon 0.6295000000000408 mean q 3.8227003\n",
      "Episode:  741 Reward: 110.0 Epsilon 0.6290000000000409 mean q 3.627175\n",
      "Episode:  742 Reward: 210.0 Epsilon 0.6285000000000409 mean q 4.0083485\n",
      "Episode:  743 Reward: 120.0 Epsilon 0.628000000000041 mean q 3.113128\n",
      "Episode:  744 Reward: 140.0 Epsilon 0.627500000000041 mean q 2.8928113\n",
      "Episode:  745 Reward: 240.0 Epsilon 0.6270000000000411 mean q 3.8966951\n",
      "Episode:  746 Reward: 140.0 Epsilon 0.6265000000000411 mean q 2.2846735\n",
      "Episode:  747 Reward: 180.0 Epsilon 0.6260000000000412 mean q 3.576505\n",
      "Episode:  748 Reward: 90.0 Epsilon 0.6255000000000412 mean q 2.3900356\n",
      "Episode:  749 Reward: 355.0 Epsilon 0.6250000000000413 mean q 3.6545365\n",
      "Episode:  750 Reward: 255.0 Epsilon 0.6245000000000414 mean q 3.3681014\n",
      "Episode:  751 Reward: 110.0 Epsilon 0.6240000000000414 mean q 3.3688853\n",
      "Episode:  752 Reward: 155.0 Epsilon 0.6235000000000415 mean q 3.469519\n",
      "Episode:  753 Reward: 125.0 Epsilon 0.6230000000000415 mean q 3.4905348\n",
      "Episode:  754 Reward: 190.0 Epsilon 0.6225000000000416 mean q 3.8146775\n",
      "Episode:  755 Reward: 335.0 Epsilon 0.6220000000000416 mean q 1.3713691\n",
      "Episode:  756 Reward: 120.0 Epsilon 0.6215000000000417 mean q 2.3817463\n",
      "Episode:  757 Reward: 120.0 Epsilon 0.6210000000000417 mean q 2.8796427\n",
      "Episode:  758 Reward: 85.0 Epsilon 0.6205000000000418 mean q 2.9434264\n",
      "Episode:  759 Reward: 295.0 Epsilon 0.6200000000000419 mean q 3.4891574\n",
      "Episode:  760 Reward: 125.0 Epsilon 0.6195000000000419 mean q 3.578778\n",
      "Episode:  761 Reward: 135.0 Epsilon 0.619000000000042 mean q 4.1494017\n",
      "Episode:  762 Reward: 80.0 Epsilon 0.618500000000042 mean q 3.6235497\n",
      "Episode:  763 Reward: 240.0 Epsilon 0.6180000000000421 mean q 2.2150867\n",
      "Episode:  764 Reward: 255.0 Epsilon 0.6175000000000421 mean q 4.4501677\n",
      "Episode:  765 Reward: 120.0 Epsilon 0.6170000000000422 mean q 3.5517547\n",
      "Episode:  766 Reward: 55.0 Epsilon 0.6165000000000422 mean q 3.5168052\n",
      "Episode:  767 Reward: 115.0 Epsilon 0.6160000000000423 mean q 3.9636555\n",
      "Episode:  768 Reward: 55.0 Epsilon 0.6155000000000423 mean q 2.6468754\n",
      "Episode:  769 Reward: 80.0 Epsilon 0.6150000000000424 mean q 3.3457797\n",
      "Episode:  770 Reward: 280.0 Epsilon 0.6145000000000425 mean q 3.3162076\n",
      "Episode:  771 Reward: 90.0 Epsilon 0.6140000000000425 mean q 2.5134535\n",
      "Episode:  772 Reward: 55.0 Epsilon 0.6135000000000426 mean q 3.907718\n",
      "Episode:  773 Reward: 60.0 Epsilon 0.6130000000000426 mean q 2.5427942\n",
      "Episode:  774 Reward: 25.0 Epsilon 0.6125000000000427 mean q 2.2339072\n",
      "Episode:  775 Reward: 75.0 Epsilon 0.6120000000000427 mean q 3.716866\n",
      "Episode:  776 Reward: 200.0 Epsilon 0.6115000000000428 mean q 3.8841777\n",
      "Episode:  777 Reward: 75.0 Epsilon 0.6110000000000428 mean q 3.2186153\n",
      "Episode:  778 Reward: 180.0 Epsilon 0.6105000000000429 mean q 3.2369797\n",
      "Episode:  779 Reward: 180.0 Epsilon 0.610000000000043 mean q 3.3171535\n",
      "Episode:  780 Reward: 465.0 Epsilon 0.609500000000043 mean q 4.4935484\n",
      "Episode:  781 Reward: 130.0 Epsilon 0.6090000000000431 mean q 3.0706959\n",
      "Episode:  782 Reward: 365.0 Epsilon 0.6085000000000431 mean q 4.7948976\n",
      "Episode:  783 Reward: 225.0 Epsilon 0.6080000000000432 mean q 2.9512653\n",
      "Episode:  784 Reward: 110.0 Epsilon 0.6075000000000432 mean q 3.0255196\n",
      "Episode:  785 Reward: 770.0 Epsilon 0.6070000000000433 mean q 3.1973631\n",
      "Episode:  786 Reward: 85.0 Epsilon 0.6065000000000433 mean q 4.378253\n",
      "Episode:  787 Reward: 140.0 Epsilon 0.6060000000000434 mean q 3.299556\n",
      "Episode:  788 Reward: 30.0 Epsilon 0.6055000000000434 mean q 2.5336103\n",
      "Episode:  789 Reward: 115.0 Epsilon 0.6050000000000435 mean q 3.50108\n",
      "Episode:  790 Reward: 210.0 Epsilon 0.6045000000000436 mean q 3.516338\n",
      "Episode:  791 Reward: 135.0 Epsilon 0.6040000000000436 mean q 2.3374107\n",
      "Episode:  792 Reward: 95.0 Epsilon 0.6035000000000437 mean q 4.083045\n",
      "Episode:  793 Reward: 430.0 Epsilon 0.6030000000000437 mean q 4.007315\n",
      "Episode:  794 Reward: 135.0 Epsilon 0.6025000000000438 mean q 3.271262\n",
      "Episode:  795 Reward: 215.0 Epsilon 0.6020000000000438 mean q 4.0716863\n",
      "Episode:  796 Reward: 65.0 Epsilon 0.6015000000000439 mean q 3.1518345\n",
      "Episode:  797 Reward: 125.0 Epsilon 0.6010000000000439 mean q 2.3902798\n",
      "Episode:  798 Reward: 155.0 Epsilon 0.600500000000044 mean q 3.1081991\n",
      "Episode:  799 Reward: 30.0 Epsilon 0.600000000000044 mean q 3.142033\n",
      "Episode:  800 Reward: 90.0 Epsilon 0.5995000000000441 mean q 3.7771628\n",
      "Episode:  801 Reward: 130.0 Epsilon 0.5990000000000442 mean q 4.170512\n",
      "Episode:  802 Reward: 140.0 Epsilon 0.5985000000000442 mean q 3.5182536\n",
      "Episode:  803 Reward: 180.0 Epsilon 0.5980000000000443 mean q 3.8657768\n",
      "Episode:  804 Reward: 125.0 Epsilon 0.5975000000000443 mean q 2.9771538\n",
      "Episode:  805 Reward: 110.0 Epsilon 0.5970000000000444 mean q 3.349289\n",
      "Episode:  806 Reward: 110.0 Epsilon 0.5965000000000444 mean q 3.169694\n",
      "Episode:  807 Reward: 180.0 Epsilon 0.5960000000000445 mean q 3.853236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  808 Reward: 35.0 Epsilon 0.5955000000000445 mean q 2.5760257\n",
      "Episode:  809 Reward: 105.0 Epsilon 0.5950000000000446 mean q 4.2481027\n",
      "Episode:  810 Reward: 180.0 Epsilon 0.5945000000000447 mean q 3.5718029\n",
      "Episode:  811 Reward: 215.0 Epsilon 0.5940000000000447 mean q 3.6053114\n",
      "Episode:  812 Reward: 180.0 Epsilon 0.5935000000000448 mean q 4.169698\n",
      "Episode:  813 Reward: 55.0 Epsilon 0.5930000000000448 mean q 3.6819904\n",
      "Episode:  814 Reward: 155.0 Epsilon 0.5925000000000449 mean q 3.5639427\n",
      "Episode:  815 Reward: 120.0 Epsilon 0.5920000000000449 mean q 3.6973443\n",
      "Episode:  816 Reward: 185.0 Epsilon 0.591500000000045 mean q 3.1262393\n",
      "Episode:  817 Reward: 80.0 Epsilon 0.591000000000045 mean q 3.593118\n",
      "Episode:  818 Reward: 160.0 Epsilon 0.5905000000000451 mean q 3.205404\n",
      "Episode:  819 Reward: 345.0 Epsilon 0.5900000000000452 mean q 3.52454\n",
      "Episode:  820 Reward: 105.0 Epsilon 0.5895000000000452 mean q 3.1293373\n",
      "Episode:  821 Reward: 155.0 Epsilon 0.5890000000000453 mean q 3.6511033\n",
      "Episode:  822 Reward: 325.0 Epsilon 0.5885000000000453 mean q 4.164566\n",
      "Episode:  823 Reward: 80.0 Epsilon 0.5880000000000454 mean q 3.3768218\n",
      "Episode:  824 Reward: 215.0 Epsilon 0.5875000000000454 mean q 4.36414\n",
      "Episode:  825 Reward: 135.0 Epsilon 0.5870000000000455 mean q 3.4816294\n",
      "Episode:  826 Reward: 160.0 Epsilon 0.5865000000000455 mean q 3.6836693\n",
      "Episode:  827 Reward: 135.0 Epsilon 0.5860000000000456 mean q 4.276971\n",
      "Episode:  828 Reward: 135.0 Epsilon 0.5855000000000457 mean q 3.0323074\n",
      "Episode:  829 Reward: 60.0 Epsilon 0.5850000000000457 mean q 3.1946647\n",
      "Episode:  830 Reward: 135.0 Epsilon 0.5845000000000458 mean q 2.4135048\n",
      "Episode:  831 Reward: 115.0 Epsilon 0.5840000000000458 mean q 3.9341247\n",
      "Episode:  832 Reward: 315.0 Epsilon 0.5835000000000459 mean q 3.8814864\n",
      "Episode:  833 Reward: 240.0 Epsilon 0.5830000000000459 mean q 4.424781\n",
      "Episode:  834 Reward: 115.0 Epsilon 0.582500000000046 mean q 4.0262613\n",
      "Episode:  835 Reward: 125.0 Epsilon 0.582000000000046 mean q 3.405248\n",
      "Episode:  836 Reward: 95.0 Epsilon 0.5815000000000461 mean q 3.3996377\n",
      "Episode:  837 Reward: 80.0 Epsilon 0.5810000000000461 mean q 3.3287091\n",
      "Episode:  838 Reward: 100.0 Epsilon 0.5805000000000462 mean q 3.0758538\n",
      "Episode:  839 Reward: 155.0 Epsilon 0.5800000000000463 mean q 2.7331543\n",
      "Episode:  840 Reward: 60.0 Epsilon 0.5795000000000463 mean q 2.3201761\n",
      "Episode:  841 Reward: 35.0 Epsilon 0.5790000000000464 mean q 3.0743644\n",
      "Episode:  842 Reward: 75.0 Epsilon 0.5785000000000464 mean q 3.795052\n",
      "Episode:  843 Reward: 120.0 Epsilon 0.5780000000000465 mean q 3.5552282\n",
      "Episode:  844 Reward: 150.0 Epsilon 0.5775000000000465 mean q 3.8404317\n",
      "Episode:  845 Reward: 105.0 Epsilon 0.5770000000000466 mean q 3.2176437\n",
      "Episode:  846 Reward: 110.0 Epsilon 0.5765000000000466 mean q 4.578923\n",
      "Episode:  847 Reward: 20.0 Epsilon 0.5760000000000467 mean q 2.3466382\n",
      "Episode:  848 Reward: 110.0 Epsilon 0.5755000000000468 mean q 2.0757346\n",
      "Episode:  849 Reward: 210.0 Epsilon 0.5750000000000468 mean q 4.219323\n",
      "Episode:  850 Reward: 80.0 Epsilon 0.5745000000000469 mean q 3.7715044\n",
      "Episode:  851 Reward: 35.0 Epsilon 0.5740000000000469 mean q 3.6050782\n",
      "Episode:  852 Reward: 215.0 Epsilon 0.573500000000047 mean q 3.8660226\n",
      "Episode:  853 Reward: 315.0 Epsilon 0.573000000000047 mean q 4.4743824\n",
      "Episode:  854 Reward: 75.0 Epsilon 0.5725000000000471 mean q 2.1549785\n",
      "Episode:  855 Reward: 315.0 Epsilon 0.5720000000000471 mean q 3.8329725\n",
      "Episode:  856 Reward: 130.0 Epsilon 0.5715000000000472 mean q 3.1917405\n",
      "Episode:  857 Reward: 255.0 Epsilon 0.5710000000000472 mean q 3.4499955\n",
      "Episode:  858 Reward: 160.0 Epsilon 0.5705000000000473 mean q 3.2131839\n",
      "Episode:  859 Reward: 195.0 Epsilon 0.5700000000000474 mean q 4.685561\n",
      "Episode:  860 Reward: 230.0 Epsilon 0.5695000000000474 mean q 3.5739226\n",
      "Episode:  861 Reward: 65.0 Epsilon 0.5690000000000475 mean q 3.1644943\n",
      "Episode:  862 Reward: 180.0 Epsilon 0.5685000000000475 mean q 3.8339689\n",
      "Episode:  863 Reward: 30.0 Epsilon 0.5680000000000476 mean q 3.2013443\n",
      "Episode:  864 Reward: 115.0 Epsilon 0.5675000000000476 mean q 2.84763\n",
      "Episode:  865 Reward: 150.0 Epsilon 0.5670000000000477 mean q 4.2239647\n",
      "Episode:  866 Reward: 110.0 Epsilon 0.5665000000000477 mean q 4.1932173\n",
      "Episode:  867 Reward: 365.0 Epsilon 0.5660000000000478 mean q 3.60446\n",
      "Episode:  868 Reward: 310.0 Epsilon 0.5655000000000479 mean q 4.2097526\n",
      "Episode:  869 Reward: 135.0 Epsilon 0.5650000000000479 mean q 3.2697084\n",
      "Episode:  870 Reward: 110.0 Epsilon 0.564500000000048 mean q 2.5027592\n",
      "Episode:  871 Reward: 10.0 Epsilon 0.564000000000048 mean q 2.4207304\n",
      "Episode:  872 Reward: 5.0 Epsilon 0.5635000000000481 mean q 3.9090083\n",
      "Episode:  873 Reward: 120.0 Epsilon 0.5630000000000481 mean q 3.0118146\n",
      "Episode:  874 Reward: 65.0 Epsilon 0.5625000000000482 mean q 3.3063352\n",
      "Episode:  875 Reward: 155.0 Epsilon 0.5620000000000482 mean q 3.879932\n",
      "Episode:  876 Reward: 215.0 Epsilon 0.5615000000000483 mean q 3.1136162\n",
      "Episode:  877 Reward: 390.0 Epsilon 0.5610000000000483 mean q 4.5762415\n",
      "Episode:  878 Reward: 180.0 Epsilon 0.5605000000000484 mean q 4.0403743\n",
      "Episode:  879 Reward: 120.0 Epsilon 0.5600000000000485 mean q 2.7191029\n",
      "Episode:  880 Reward: 240.0 Epsilon 0.5595000000000485 mean q 4.0550547\n",
      "Episode:  881 Reward: 135.0 Epsilon 0.5590000000000486 mean q 3.1943665\n",
      "Episode:  882 Reward: 40.0 Epsilon 0.5585000000000486 mean q 3.351613\n",
      "Episode:  883 Reward: 170.0 Epsilon 0.5580000000000487 mean q 4.6091094\n",
      "Episode:  884 Reward: 110.0 Epsilon 0.5575000000000487 mean q 2.6243217\n",
      "Episode:  885 Reward: 135.0 Epsilon 0.5570000000000488 mean q 2.8275893\n",
      "Episode:  886 Reward: 65.0 Epsilon 0.5565000000000488 mean q 2.848374\n",
      "Episode:  887 Reward: 55.0 Epsilon 0.5560000000000489 mean q 3.1927915\n",
      "Episode:  888 Reward: 210.0 Epsilon 0.555500000000049 mean q 4.3739676\n",
      "Episode:  889 Reward: 120.0 Epsilon 0.555000000000049 mean q 3.094595\n",
      "Episode:  890 Reward: 100.0 Epsilon 0.5545000000000491 mean q 2.8804386\n",
      "Episode:  891 Reward: 150.0 Epsilon 0.5540000000000491 mean q 3.0474353\n",
      "Episode:  892 Reward: 305.0 Epsilon 0.5535000000000492 mean q 4.032238\n",
      "Episode:  893 Reward: 135.0 Epsilon 0.5530000000000492 mean q 3.9585876\n",
      "Episode:  894 Reward: 70.0 Epsilon 0.5525000000000493 mean q 2.7850337\n",
      "Episode:  895 Reward: 160.0 Epsilon 0.5520000000000493 mean q 3.2773113\n",
      "Episode:  896 Reward: 185.0 Epsilon 0.5515000000000494 mean q 3.9922872\n",
      "Episode:  897 Reward: 215.0 Epsilon 0.5510000000000495 mean q 3.4748259\n",
      "Episode:  898 Reward: 120.0 Epsilon 0.5505000000000495 mean q 4.080429\n",
      "Episode:  899 Reward: 140.0 Epsilon 0.5500000000000496 mean q 3.482251\n",
      "Episode:  900 Reward: 80.0 Epsilon 0.5495000000000496 mean q 4.355116\n",
      "Episode:  901 Reward: 210.0 Epsilon 0.5490000000000497 mean q 2.3536172\n",
      "Episode:  902 Reward: 160.0 Epsilon 0.5485000000000497 mean q 3.6746702\n",
      "Episode:  903 Reward: 45.0 Epsilon 0.5480000000000498 mean q 3.3420866\n",
      "Episode:  904 Reward: 185.0 Epsilon 0.5475000000000498 mean q 4.307653\n",
      "Episode:  905 Reward: 120.0 Epsilon 0.5470000000000499 mean q 3.5510561\n",
      "Episode:  906 Reward: 215.0 Epsilon 0.54650000000005 mean q 2.905696\n",
      "Episode:  907 Reward: 180.0 Epsilon 0.54600000000005 mean q 3.1998394\n",
      "Episode:  908 Reward: 65.0 Epsilon 0.5455000000000501 mean q 3.362163\n",
      "Episode:  909 Reward: 135.0 Epsilon 0.5450000000000501 mean q 4.263133\n",
      "Episode:  910 Reward: 140.0 Epsilon 0.5445000000000502 mean q 2.9964898\n",
      "Episode:  911 Reward: 240.0 Epsilon 0.5440000000000502 mean q 4.3252935\n",
      "Episode:  912 Reward: 530.0 Epsilon 0.5435000000000503 mean q 4.188993\n",
      "Episode:  913 Reward: 140.0 Epsilon 0.5430000000000503 mean q 3.5555732\n",
      "Episode:  914 Reward: 145.0 Epsilon 0.5425000000000504 mean q 3.3147223\n",
      "Episode:  915 Reward: 155.0 Epsilon 0.5420000000000504 mean q 3.4707854\n",
      "Episode:  916 Reward: 410.0 Epsilon 0.5415000000000505 mean q 4.169449\n",
      "Episode:  917 Reward: 160.0 Epsilon 0.5410000000000506 mean q 4.103968\n",
      "Episode:  918 Reward: 135.0 Epsilon 0.5405000000000506 mean q 4.2378654\n",
      "Episode:  919 Reward: 165.0 Epsilon 0.5400000000000507 mean q 4.1231465\n",
      "Episode:  920 Reward: 160.0 Epsilon 0.5395000000000507 mean q 3.841917\n",
      "Episode:  921 Reward: 125.0 Epsilon 0.5390000000000508 mean q 3.7716327\n",
      "Episode:  922 Reward: 140.0 Epsilon 0.5385000000000508 mean q 3.9088705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  923 Reward: 120.0 Epsilon 0.5380000000000509 mean q 4.1033115\n",
      "Episode:  924 Reward: 410.0 Epsilon 0.5375000000000509 mean q 4.1809964\n",
      "Episode:  925 Reward: 215.0 Epsilon 0.537000000000051 mean q 4.3713913\n",
      "Episode:  926 Reward: 285.0 Epsilon 0.536500000000051 mean q 3.976078\n",
      "Episode:  927 Reward: 40.0 Epsilon 0.5360000000000511 mean q 2.7672377\n",
      "Episode:  928 Reward: 230.0 Epsilon 0.5355000000000512 mean q 3.872207\n",
      "Episode:  929 Reward: 490.0 Epsilon 0.5350000000000512 mean q 4.1413636\n",
      "Episode:  930 Reward: 180.0 Epsilon 0.5345000000000513 mean q 4.268289\n",
      "Episode:  931 Reward: 130.0 Epsilon 0.5340000000000513 mean q 3.6997678\n",
      "Episode:  932 Reward: 20.0 Epsilon 0.5335000000000514 mean q 2.177476\n",
      "Episode:  933 Reward: 240.0 Epsilon 0.5330000000000514 mean q 2.2132342\n",
      "Episode:  934 Reward: 120.0 Epsilon 0.5325000000000515 mean q 2.2878726\n",
      "Episode:  935 Reward: 150.0 Epsilon 0.5320000000000515 mean q 3.4334815\n",
      "Episode:  936 Reward: 130.0 Epsilon 0.5315000000000516 mean q 3.327064\n",
      "Episode:  937 Reward: 105.0 Epsilon 0.5310000000000517 mean q 3.5684376\n",
      "Episode:  938 Reward: 75.0 Epsilon 0.5305000000000517 mean q 3.4651732\n",
      "Episode:  939 Reward: 480.0 Epsilon 0.5300000000000518 mean q 3.6845508\n",
      "Episode:  940 Reward: 10.0 Epsilon 0.5295000000000518 mean q 2.3873005\n",
      "Episode:  941 Reward: 240.0 Epsilon 0.5290000000000519 mean q 3.9954433\n",
      "Episode:  942 Reward: 110.0 Epsilon 0.5285000000000519 mean q 3.6527975\n",
      "Episode:  943 Reward: 110.0 Epsilon 0.528000000000052 mean q 2.9541223\n",
      "Episode:  944 Reward: 135.0 Epsilon 0.527500000000052 mean q 3.0010288\n",
      "Episode:  945 Reward: 135.0 Epsilon 0.5270000000000521 mean q 3.0098403\n",
      "Episode:  946 Reward: 45.0 Epsilon 0.5265000000000521 mean q 1.9654236\n",
      "Episode:  947 Reward: 210.0 Epsilon 0.5260000000000522 mean q 3.846688\n",
      "Episode:  948 Reward: 45.0 Epsilon 0.5255000000000523 mean q 3.1325867\n",
      "Episode:  949 Reward: 105.0 Epsilon 0.5250000000000523 mean q 3.651477\n",
      "Episode:  950 Reward: 120.0 Epsilon 0.5245000000000524 mean q 2.1783528\n",
      "Episode:  951 Reward: 55.0 Epsilon 0.5240000000000524 mean q 3.0591137\n",
      "Episode:  952 Reward: 165.0 Epsilon 0.5235000000000525 mean q 3.549578\n",
      "Episode:  953 Reward: 210.0 Epsilon 0.5230000000000525 mean q 2.6154947\n",
      "Episode:  954 Reward: 75.0 Epsilon 0.5225000000000526 mean q 3.212093\n",
      "Episode:  955 Reward: 55.0 Epsilon 0.5220000000000526 mean q 2.4943378\n",
      "Episode:  956 Reward: 115.0 Epsilon 0.5215000000000527 mean q 3.9230723\n",
      "Episode:  957 Reward: 55.0 Epsilon 0.5210000000000528 mean q 2.8403869\n",
      "Episode:  958 Reward: 75.0 Epsilon 0.5205000000000528 mean q 3.1165707\n",
      "Episode:  959 Reward: 210.0 Epsilon 0.5200000000000529 mean q 3.9006233\n",
      "Episode:  960 Reward: 110.0 Epsilon 0.5195000000000529 mean q 2.9136767\n",
      "Episode:  961 Reward: 85.0 Epsilon 0.519000000000053 mean q 3.375888\n",
      "Episode:  962 Reward: 150.0 Epsilon 0.518500000000053 mean q 3.546773\n",
      "Episode:  963 Reward: 325.0 Epsilon 0.5180000000000531 mean q 4.006919\n",
      "Episode:  964 Reward: 210.0 Epsilon 0.5175000000000531 mean q 4.078578\n",
      "Episode:  965 Reward: 45.0 Epsilon 0.5170000000000532 mean q 3.3171036\n",
      "Episode:  966 Reward: 75.0 Epsilon 0.5165000000000532 mean q 3.1971295\n",
      "Episode:  967 Reward: 70.0 Epsilon 0.5160000000000533 mean q 4.068376\n",
      "Episode:  968 Reward: 110.0 Epsilon 0.5155000000000534 mean q 2.6765957\n",
      "Episode:  969 Reward: 90.0 Epsilon 0.5150000000000534 mean q 3.835335\n",
      "Episode:  970 Reward: 215.0 Epsilon 0.5145000000000535 mean q 3.6141598\n",
      "Episode:  971 Reward: 200.0 Epsilon 0.5140000000000535 mean q 3.7379615\n",
      "Episode:  972 Reward: 80.0 Epsilon 0.5135000000000536 mean q 4.09151\n",
      "Episode:  973 Reward: 155.0 Epsilon 0.5130000000000536 mean q 4.0568023\n",
      "Episode:  974 Reward: 135.0 Epsilon 0.5125000000000537 mean q 3.2103393\n",
      "Episode:  975 Reward: 35.0 Epsilon 0.5120000000000537 mean q 2.007009\n",
      "Episode:  976 Reward: 115.0 Epsilon 0.5115000000000538 mean q 3.437115\n",
      "Episode:  977 Reward: 35.0 Epsilon 0.5110000000000539 mean q 2.5142202\n",
      "Episode:  978 Reward: 180.0 Epsilon 0.5105000000000539 mean q 2.5764196\n",
      "Episode:  979 Reward: 110.0 Epsilon 0.510000000000054 mean q 1.3605866\n",
      "Episode:  980 Reward: 235.0 Epsilon 0.509500000000054 mean q 3.9665248\n",
      "Episode:  981 Reward: 90.0 Epsilon 0.5090000000000541 mean q 3.2518632\n",
      "Episode:  982 Reward: 80.0 Epsilon 0.5085000000000541 mean q 3.0348036\n",
      "Episode:  983 Reward: 100.0 Epsilon 0.5080000000000542 mean q 4.178378\n",
      "Episode:  984 Reward: 485.0 Epsilon 0.5075000000000542 mean q 4.361659\n",
      "Episode:  985 Reward: 250.0 Epsilon 0.5070000000000543 mean q 3.9219253\n",
      "Episode:  986 Reward: 180.0 Epsilon 0.5065000000000544 mean q 3.2298322\n",
      "Episode:  987 Reward: 150.0 Epsilon 0.5060000000000544 mean q 4.3788824\n",
      "Episode:  988 Reward: 155.0 Epsilon 0.5055000000000545 mean q 3.2235327\n",
      "Episode:  989 Reward: 110.0 Epsilon 0.5050000000000545 mean q 3.2892532\n",
      "Episode:  990 Reward: 155.0 Epsilon 0.5045000000000546 mean q 4.15594\n",
      "Episode:  991 Reward: 55.0 Epsilon 0.5040000000000546 mean q 3.2785459\n",
      "Episode:  992 Reward: 65.0 Epsilon 0.5035000000000547 mean q 3.0488298\n",
      "Episode:  993 Reward: 80.0 Epsilon 0.5030000000000547 mean q 3.2456322\n",
      "Episode:  994 Reward: 110.0 Epsilon 0.5025000000000548 mean q 2.9233124\n",
      "Episode:  995 Reward: 285.0 Epsilon 0.5020000000000548 mean q 3.7250047\n",
      "Episode:  996 Reward: 210.0 Epsilon 0.5015000000000549 mean q 3.712917\n",
      "Episode:  997 Reward: 270.0 Epsilon 0.501000000000055 mean q 4.0308943\n",
      "Episode:  998 Reward: 135.0 Epsilon 0.500500000000055 mean q 3.2021189\n",
      "Episode:  999 Reward: 230.0 Epsilon 0.5000000000000551 mean q 4.1562767\n",
      "Episode:  1000 Reward: 155.0 Epsilon 0.49950000000005507 mean q 4.352307\n",
      "Episode:  1001 Reward: 210.0 Epsilon 0.49900000000005507 mean q 2.0129032\n",
      "Episode:  1002 Reward: 180.0 Epsilon 0.49850000000005507 mean q 4.098938\n",
      "Episode:  1003 Reward: 60.0 Epsilon 0.49800000000005507 mean q 2.4301708\n",
      "Episode:  1004 Reward: 240.0 Epsilon 0.49750000000005506 mean q 4.568812\n",
      "Episode:  1005 Reward: 315.0 Epsilon 0.49700000000005506 mean q 3.8649895\n",
      "Episode:  1006 Reward: 155.0 Epsilon 0.49650000000005506 mean q 3.5345848\n",
      "Episode:  1007 Reward: 185.0 Epsilon 0.49600000000005506 mean q 3.7205417\n",
      "Episode:  1008 Reward: 55.0 Epsilon 0.49550000000005506 mean q 2.8525639\n",
      "Episode:  1009 Reward: 85.0 Epsilon 0.49500000000005506 mean q 3.3771584\n",
      "Episode:  1010 Reward: 135.0 Epsilon 0.49450000000005506 mean q 3.2212198\n",
      "Episode:  1011 Reward: 65.0 Epsilon 0.49400000000005506 mean q 2.8259423\n",
      "Episode:  1012 Reward: 35.0 Epsilon 0.49350000000005506 mean q 2.374354\n",
      "Episode:  1013 Reward: 240.0 Epsilon 0.49300000000005506 mean q 4.2345467\n",
      "Episode:  1014 Reward: 0.0 Epsilon 0.49250000000005506 mean q 2.704385\n",
      "Episode:  1015 Reward: 100.0 Epsilon 0.49200000000005506 mean q 3.2130086\n",
      "Episode:  1016 Reward: 120.0 Epsilon 0.49150000000005506 mean q 2.8908248\n",
      "Episode:  1017 Reward: 110.0 Epsilon 0.49100000000005506 mean q 3.0544298\n",
      "Episode:  1018 Reward: 80.0 Epsilon 0.49050000000005506 mean q 3.195287\n",
      "Episode:  1019 Reward: 45.0 Epsilon 0.49000000000005506 mean q 3.525151\n",
      "Episode:  1020 Reward: 155.0 Epsilon 0.48950000000005506 mean q 3.5780752\n",
      "Episode:  1021 Reward: 170.0 Epsilon 0.48900000000005506 mean q 3.6344647\n",
      "Episode:  1022 Reward: 155.0 Epsilon 0.48850000000005506 mean q 3.3902497\n",
      "Episode:  1023 Reward: 120.0 Epsilon 0.48800000000005506 mean q 3.207397\n",
      "Episode:  1024 Reward: 150.0 Epsilon 0.48750000000005506 mean q 3.9076195\n",
      "Episode:  1025 Reward: 45.0 Epsilon 0.48700000000005506 mean q 2.7844157\n",
      "Episode:  1026 Reward: 510.0 Epsilon 0.48650000000005506 mean q 4.4638033\n",
      "Episode:  1027 Reward: 50.0 Epsilon 0.48600000000005505 mean q 2.6989355\n",
      "Episode:  1028 Reward: 110.0 Epsilon 0.48550000000005505 mean q 2.61989\n",
      "Episode:  1029 Reward: 425.0 Epsilon 0.48500000000005505 mean q 3.9295695\n",
      "Episode:  1030 Reward: 65.0 Epsilon 0.48450000000005505 mean q 3.7200944\n",
      "Episode:  1031 Reward: 210.0 Epsilon 0.48400000000005505 mean q 3.5278955\n",
      "Episode:  1032 Reward: 125.0 Epsilon 0.48350000000005505 mean q 3.3818748\n",
      "Episode:  1033 Reward: 210.0 Epsilon 0.48300000000005505 mean q 4.0280843\n",
      "Episode:  1034 Reward: 35.0 Epsilon 0.48250000000005505 mean q 1.6618142\n",
      "Episode:  1035 Reward: 150.0 Epsilon 0.48200000000005505 mean q 3.8894725\n",
      "Episode:  1036 Reward: 225.0 Epsilon 0.48150000000005505 mean q 3.2826955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1037 Reward: 80.0 Epsilon 0.48100000000005505 mean q 3.6107576\n",
      "Episode:  1038 Reward: 65.0 Epsilon 0.48050000000005505 mean q 2.6536047\n",
      "Episode:  1039 Reward: 260.0 Epsilon 0.48000000000005505 mean q 4.2821326\n",
      "Episode:  1040 Reward: 160.0 Epsilon 0.47950000000005505 mean q 3.8079026\n",
      "Episode:  1041 Reward: 180.0 Epsilon 0.47900000000005505 mean q 3.0490625\n",
      "Episode:  1042 Reward: 215.0 Epsilon 0.47850000000005505 mean q 2.6942132\n",
      "Episode:  1043 Reward: 35.0 Epsilon 0.47800000000005505 mean q 3.1254897\n",
      "Episode:  1044 Reward: 110.0 Epsilon 0.47750000000005505 mean q 2.9949934\n",
      "Episode:  1045 Reward: 180.0 Epsilon 0.47700000000005505 mean q 3.6049907\n",
      "Episode:  1046 Reward: 120.0 Epsilon 0.47650000000005505 mean q 3.7180412\n",
      "Episode:  1047 Reward: 410.0 Epsilon 0.47600000000005505 mean q 3.4990737\n",
      "Episode:  1048 Reward: 35.0 Epsilon 0.47550000000005505 mean q 2.4229805\n",
      "Episode:  1049 Reward: 150.0 Epsilon 0.47500000000005504 mean q 3.314945\n",
      "Episode:  1050 Reward: 135.0 Epsilon 0.47450000000005504 mean q 4.529733\n",
      "Episode:  1051 Reward: 640.0 Epsilon 0.47400000000005504 mean q 3.325589\n",
      "Episode:  1052 Reward: 60.0 Epsilon 0.47350000000005504 mean q 3.097124\n",
      "Episode:  1053 Reward: 150.0 Epsilon 0.47300000000005504 mean q 3.419027\n",
      "Episode:  1054 Reward: 20.0 Epsilon 0.47250000000005504 mean q 2.9773128\n",
      "Episode:  1055 Reward: 125.0 Epsilon 0.47200000000005504 mean q 3.15876\n",
      "Episode:  1056 Reward: 125.0 Epsilon 0.47150000000005504 mean q 2.4581892\n",
      "Episode:  1057 Reward: 210.0 Epsilon 0.47100000000005504 mean q 3.8054512\n",
      "Episode:  1058 Reward: 80.0 Epsilon 0.47050000000005504 mean q 3.8666182\n",
      "Episode:  1059 Reward: 185.0 Epsilon 0.47000000000005504 mean q 3.254592\n",
      "Episode:  1060 Reward: 350.0 Epsilon 0.46950000000005504 mean q 3.9632063\n",
      "Episode:  1061 Reward: 85.0 Epsilon 0.46900000000005504 mean q 3.3546576\n",
      "Episode:  1062 Reward: 120.0 Epsilon 0.46850000000005504 mean q 2.856472\n",
      "Episode:  1063 Reward: 20.0 Epsilon 0.46800000000005504 mean q 2.9434078\n",
      "Episode:  1064 Reward: 265.0 Epsilon 0.46750000000005504 mean q 4.358393\n",
      "Episode:  1065 Reward: 185.0 Epsilon 0.46700000000005504 mean q 4.685124\n",
      "Episode:  1066 Reward: 105.0 Epsilon 0.46650000000005504 mean q 2.3725095\n",
      "Episode:  1067 Reward: 455.0 Epsilon 0.46600000000005504 mean q 5.017238\n",
      "Episode:  1068 Reward: 105.0 Epsilon 0.46550000000005504 mean q 1.6338099\n",
      "Episode:  1069 Reward: 200.0 Epsilon 0.46500000000005504 mean q 3.839307\n",
      "Episode:  1070 Reward: 100.0 Epsilon 0.46450000000005504 mean q 3.3580034\n",
      "Episode:  1071 Reward: 80.0 Epsilon 0.46400000000005504 mean q 3.116211\n",
      "Episode:  1072 Reward: 265.0 Epsilon 0.46350000000005503 mean q 4.431839\n",
      "Episode:  1073 Reward: 135.0 Epsilon 0.46300000000005503 mean q 2.8047318\n",
      "Episode:  1074 Reward: 255.0 Epsilon 0.46250000000005503 mean q 4.1664867\n",
      "Episode:  1075 Reward: 155.0 Epsilon 0.46200000000005503 mean q 3.4608302\n",
      "Episode:  1076 Reward: 110.0 Epsilon 0.46150000000005503 mean q 2.735883\n",
      "Episode:  1077 Reward: 195.0 Epsilon 0.46100000000005503 mean q 3.994757\n",
      "Episode:  1078 Reward: 190.0 Epsilon 0.46050000000005503 mean q 4.7198277\n",
      "Episode:  1079 Reward: 70.0 Epsilon 0.46000000000005503 mean q 4.0040984\n",
      "Episode:  1080 Reward: 185.0 Epsilon 0.45950000000005503 mean q 4.119517\n",
      "Episode:  1081 Reward: 125.0 Epsilon 0.45900000000005503 mean q 4.0867147\n",
      "Episode:  1082 Reward: 50.0 Epsilon 0.45850000000005503 mean q 3.257261\n",
      "Episode:  1083 Reward: 175.0 Epsilon 0.45800000000005503 mean q 3.3177462\n",
      "Episode:  1084 Reward: 120.0 Epsilon 0.45750000000005503 mean q 2.0101733\n",
      "Episode:  1085 Reward: 105.0 Epsilon 0.45700000000005503 mean q 3.7271714\n",
      "Episode:  1086 Reward: 165.0 Epsilon 0.45650000000005503 mean q 3.8407335\n",
      "Episode:  1087 Reward: 180.0 Epsilon 0.45600000000005503 mean q 3.8872116\n",
      "Episode:  1088 Reward: 80.0 Epsilon 0.455500000000055 mean q 3.1130342\n",
      "Episode:  1089 Reward: 45.0 Epsilon 0.455000000000055 mean q 2.86092\n",
      "Episode:  1090 Reward: 80.0 Epsilon 0.454500000000055 mean q 2.2820969\n",
      "Episode:  1091 Reward: 135.0 Epsilon 0.454000000000055 mean q 3.7550237\n",
      "Episode:  1092 Reward: 135.0 Epsilon 0.453500000000055 mean q 2.7215083\n",
      "Episode:  1093 Reward: 120.0 Epsilon 0.453000000000055 mean q 2.962574\n",
      "Episode:  1094 Reward: 115.0 Epsilon 0.452500000000055 mean q 4.568402\n",
      "Episode:  1095 Reward: 200.0 Epsilon 0.452000000000055 mean q 3.1241415\n",
      "Episode:  1096 Reward: 90.0 Epsilon 0.451500000000055 mean q 3.0238142\n",
      "Episode:  1097 Reward: 95.0 Epsilon 0.451000000000055 mean q 3.8092659\n",
      "Episode:  1098 Reward: 70.0 Epsilon 0.450500000000055 mean q 2.697147\n",
      "Episode:  1099 Reward: 125.0 Epsilon 0.450000000000055 mean q 3.503227\n",
      "Episode:  1100 Reward: 100.0 Epsilon 0.449500000000055 mean q 3.0681353\n",
      "Episode:  1101 Reward: 80.0 Epsilon 0.449000000000055 mean q 3.4405246\n",
      "Episode:  1102 Reward: 350.0 Epsilon 0.448500000000055 mean q 3.7772753\n",
      "Episode:  1103 Reward: 155.0 Epsilon 0.448000000000055 mean q 3.6479821\n",
      "Episode:  1104 Reward: 85.0 Epsilon 0.447500000000055 mean q 4.1450357\n",
      "Episode:  1105 Reward: 110.0 Epsilon 0.447000000000055 mean q 4.0077934\n",
      "Episode:  1106 Reward: 225.0 Epsilon 0.446500000000055 mean q 4.4213314\n",
      "Episode:  1107 Reward: 105.0 Epsilon 0.446000000000055 mean q 3.3410935\n",
      "Episode:  1108 Reward: 230.0 Epsilon 0.445500000000055 mean q 4.151156\n",
      "Episode:  1109 Reward: 215.0 Epsilon 0.445000000000055 mean q 4.000474\n",
      "Episode:  1110 Reward: 155.0 Epsilon 0.444500000000055 mean q 3.5726256\n",
      "Episode:  1111 Reward: 120.0 Epsilon 0.444000000000055 mean q 3.1731915\n",
      "Episode:  1112 Reward: 185.0 Epsilon 0.443500000000055 mean q 3.5381756\n",
      "Episode:  1113 Reward: 90.0 Epsilon 0.443000000000055 mean q 3.7479646\n",
      "Episode:  1114 Reward: 130.0 Epsilon 0.442500000000055 mean q 3.9095633\n",
      "Episode:  1115 Reward: 35.0 Epsilon 0.442000000000055 mean q 2.0390735\n",
      "Episode:  1116 Reward: 205.0 Epsilon 0.441500000000055 mean q 3.4512663\n",
      "Episode:  1117 Reward: 120.0 Epsilon 0.441000000000055 mean q 3.5432782\n",
      "Episode:  1118 Reward: 175.0 Epsilon 0.440500000000055 mean q 3.1754797\n",
      "Episode:  1119 Reward: 155.0 Epsilon 0.440000000000055 mean q 3.5788777\n",
      "Episode:  1120 Reward: 50.0 Epsilon 0.439500000000055 mean q 2.096928\n",
      "Episode:  1121 Reward: 175.0 Epsilon 0.439000000000055 mean q 3.765472\n",
      "Episode:  1122 Reward: 210.0 Epsilon 0.438500000000055 mean q 4.0347533\n",
      "Episode:  1123 Reward: 80.0 Epsilon 0.438000000000055 mean q 3.153671\n",
      "Episode:  1124 Reward: 110.0 Epsilon 0.437500000000055 mean q 2.272673\n",
      "Episode:  1125 Reward: 45.0 Epsilon 0.437000000000055 mean q 2.5282214\n",
      "Episode:  1126 Reward: 180.0 Epsilon 0.436500000000055 mean q 3.735518\n",
      "Episode:  1127 Reward: 150.0 Epsilon 0.436000000000055 mean q 3.686705\n",
      "Episode:  1128 Reward: 215.0 Epsilon 0.435500000000055 mean q 3.3100123\n",
      "Episode:  1129 Reward: 420.0 Epsilon 0.435000000000055 mean q 4.1855097\n",
      "Episode:  1130 Reward: 50.0 Epsilon 0.434500000000055 mean q 3.9821026\n",
      "Episode:  1131 Reward: 185.0 Epsilon 0.434000000000055 mean q 3.7775865\n",
      "Episode:  1132 Reward: 90.0 Epsilon 0.433500000000055 mean q 2.1059732\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b206e88d0a22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop_ddqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-003a42e52619>\u001b[0m in \u001b[0;36mtrain_loop_ddqn\u001b[1;34m(model, env, num_episodes, batch_size, gamma)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# if buffer contains more than 1000 samples, perform one training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_length\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sample a minibatch of transitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mq_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_q_values_for_both_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mtd_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_td_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Åk 5\\Deep Machine Learning\\deep-machine-learning\\Project\\Space_invader\\dqn_model.py\u001b[0m in \u001b[0;36msample_minibatch\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mnext_state_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mstate_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0maction_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mreward_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"SpaceInvaders-ram-v0\")\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Our Neural Netork model used to estimate the Q-values\n",
    "model = DoubleQLearningModel(state_dim=obs_dim, action_dim=num_actions, learning_rate=1e-4)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(state_size=obs_dim)\n",
    "\n",
    "# Train\n",
    "num_episodes = 2000 \n",
    "batch_size = 128 \n",
    "R, R_avg = train_loop_ddqn(model, env, num_episodes, batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_episodes = 1\n",
    "env = gym.make(\"SpaceInvaders-ram-v0\")\n",
    "\n",
    "for i in range(num_episodes):\n",
    "        state = env.reset() #reset to initial state\n",
    "        state = np.expand_dims(state, axis=0)/2\n",
    "        terminal = False # reset terminal flag\n",
    "        while not terminal:\n",
    "            env.render()\n",
    "            time.sleep(.05)\n",
    "            q_values = model.get_q_values(state)\n",
    "            policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "            action = np.random.choice(num_actions, p=policy)\n",
    "            state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "            state = np.expand_dims(state, axis=0)/2\n",
    "# close window\n",
    "env.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-10e9a1758426>:81: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Elapsed time(s):  0\n",
      "Elapsed time(s):  1\n",
      "Elapsed time(s):  2\n",
      "Elapsed time(s):  3\n",
      "Elapsed time(s):  4\n",
      "Elapsed time(s):  5\n",
      "Total R: 50.0\n",
      "Elapsed time(s):  6\n",
      "Elapsed time(s):  7\n",
      "Elapsed time(s):  8\n",
      "Total R: 50.0\n",
      "Elapsed time(s):  9\n",
      "Total R: 50.0\n",
      "Total R: 30.0\n",
      "Elapsed time(s):  10\n",
      "Total R: 55.0\n",
      "Elapsed time(s):  11\n",
      "Total R: 120.0\n",
      "Total R: 135.0\n",
      "Elapsed time(s):  12\n",
      "Elapsed time(s):  13\n",
      "Elapsed time(s):  14\n",
      "Total R: 105.0\n",
      "Elapsed time(s):  15\n",
      "Elapsed time(s):  16\n",
      "Elapsed time(s):  17\n",
      "Total R: 80.0\n",
      "Total R: 255.0\n",
      "Elapsed time(s):  18\n",
      "Total R: 125.0\n",
      "Total R: 110.0\n",
      "Elapsed time(s):  19\n",
      "Elapsed time(s):  20\n",
      "Elapsed time(s):  21\n",
      "Total R: 110.0\n",
      "Elapsed time(s):  22\n",
      "Elapsed time(s):  23\n",
      "Elapsed time(s):  24\n",
      "Total R: 80.0\n",
      "Elapsed time(s):  25\n",
      "Elapsed time(s):  26\n",
      "Elapsed time(s):  27\n",
      "Elapsed time(s):  28\n",
      "Total R: 130.0\n",
      "Total R: 415.0\n",
      "Elapsed time(s):  29\n",
      "Total R: 125.0\n",
      "Total R: 210.0\n",
      "Total R: 80.0\n",
      "Total R: 0.0\n",
      "Total R: 185.0\n",
      "Total R: 255.0\n",
      "Total R: 85.0\n",
      "Total R: 0.0\n",
      "Training finished\n",
      "Total R: 155.0\n",
      "Total R: 110.0\n",
      "Total R: 180.0\n",
      "Total R: 480.0\n",
      "Total R: 110.0\n",
      "Total R: 60.0\n",
      "Total R: 410.0\n",
      "Total R: 135.0\n",
      "Total R: 105.0\n",
      "Total R: 35.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-10e9a1758426>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training finished\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m \u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-10e9a1758426>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_signal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunEpisode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-10e9a1758426>\u001b[0m in \u001b[0;36mrunEpisode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTHREAD_DELAY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# yield\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dml\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dml\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dml\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, arr)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dml\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mblit\u001b[1;34m(self, x, y, z, width, height)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_texture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mblit_to_texture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternalformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dml\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mget_texture\u001b[1;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    857\u001b[0m             self._current_texture = self.create_texture(Texture,\n\u001b[0;32m    858\u001b[0m                                                         \u001b[0mrectangle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                                                         force_rectangle)\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_texture\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dml\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mcreate_texture\u001b[1;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[0minternalformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_internalformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         texture = cls.create(self.width, self.height, internalformat,\n\u001b[1;32m--> 844\u001b[1;33m                              rectangle, force_rectangle)\n\u001b[0m\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mtexture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dml\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                      \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1563\u001b[0m                      \u001b[0mGL_RGBA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGL_UNSIGNED_BYTE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                      blank)\n\u001b[0m\u001b[0;32m   1565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m         \u001b[0mtexture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexture_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexture_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dml\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No GL context; create a Window first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# OpenGym CartPole-v0 with A3C on GPU\n",
    "# -----------------------------------\n",
    "#\n",
    "# A3C implementation with GPU optimizer threads.\n",
    "# \n",
    "# Made as part of blog series Let's make an A3C, available at\n",
    "# https://jaromiru.com/2017/02/16/lets-make-an-a3c-theory/\n",
    "#\n",
    "# author: Jaromir Janisch, 2017\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym, time, random, threading\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "\n",
    "#-- constants\n",
    "ENV = 'SpaceInvaders-ram-v0'\n",
    "\n",
    "RUN_TIME = 30\n",
    "THREADS = 8\n",
    "OPTIMIZERS = 2\n",
    "THREAD_DELAY = 0.001\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "N_STEP_RETURN = 8\n",
    "GAMMA_N = GAMMA ** N_STEP_RETURN\n",
    "\n",
    "EPS_START = 0.4\n",
    "EPS_STOP  = .15\n",
    "EPS_STEPS = 75000\n",
    "\n",
    "MIN_BATCH = 32\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "LOSS_V = .5\t\t\t# v loss coefficient\n",
    "LOSS_ENTROPY = .01 \t# entropy coefficient\n",
    "\n",
    "#---------\n",
    "class Brain:\n",
    "    train_queue = [ [], [], [], [], [] ]\t# s, a, r, s', s' terminal mask\n",
    "    lock_queue = threading.Lock()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = tf.Session()\n",
    "        K.set_session(self.session)\n",
    "        K.manual_variable_initialization(True)\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.graph = self._build_graph(self.model)\n",
    "\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.default_graph = tf.get_default_graph()\n",
    "\n",
    "        self.default_graph.finalize()\t# avoid modifications\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        l_input = Input( batch_shape=(None, NUM_STATE) )\n",
    "        l_dense = Dense(16, activation='relu')(l_input)\n",
    "\n",
    "        out_actions = Dense(NUM_ACTIONS, activation='softmax')(l_dense)\n",
    "        out_value   = Dense(1, activation='linear')(l_dense)\n",
    "\n",
    "        model = Model(inputs=[l_input], outputs=[out_actions, out_value])\n",
    "        model._make_predict_function()\t# have to initialize before threading\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_graph(self, model):\n",
    "        s_t = tf.placeholder(tf.float32, shape=(None, NUM_STATE))\n",
    "        a_t = tf.placeholder(tf.float32, shape=(None, NUM_ACTIONS))\n",
    "        r_t = tf.placeholder(tf.float32, shape=(None, 1)) # not immediate, but discounted n step reward\n",
    "        \n",
    "        p, v = model(s_t)\n",
    "\n",
    "        log_prob = tf.log( tf.reduce_sum(p * a_t, axis=1, keep_dims=True) + 1e-10)\n",
    "        advantage = r_t - v\n",
    "\n",
    "        loss_policy = - log_prob * tf.stop_gradient(advantage)\t\t\t\t\t\t\t\t\t# maximize policy\n",
    "        loss_value  = LOSS_V * tf.square(advantage)\t\t\t\t\t\t\t\t\t\t\t\t# minimize value error\n",
    "        entropy = LOSS_ENTROPY * tf.reduce_sum(p * tf.log(p + 1e-10), axis=1, keep_dims=True)\t# maximize entropy (regularization)\n",
    "\n",
    "        loss_total = tf.reduce_mean(loss_policy + loss_value + entropy)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, decay=.99)\n",
    "        minimize = optimizer.minimize(loss_total)\n",
    "\n",
    "        return s_t, a_t, r_t, minimize\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.train_queue[0]) < MIN_BATCH:\n",
    "            time.sleep(0)\t# yield\n",
    "            return\n",
    "\n",
    "        with self.lock_queue:\n",
    "            if len(self.train_queue[0]) < MIN_BATCH:\t# more thread could have passed without lock\n",
    "                return \t\t\t\t\t\t\t\t\t# we can't yield inside lock\n",
    "\n",
    "            s, a, r, s_, s_mask = self.train_queue\n",
    "            self.train_queue = [ [], [], [], [], [] ]\n",
    "\n",
    "        s = np.vstack(s)\n",
    "        a = np.vstack(a)\n",
    "        r = np.vstack(r)\n",
    "        s_ = np.vstack(s_)\n",
    "        s_mask = np.vstack(s_mask)\n",
    "\n",
    "        if len(s) > 5*MIN_BATCH: print(\"Optimizer alert! Minimizing batch of %d\" % len(s))\n",
    "\n",
    "        v = self.predict_v(s_)\n",
    "        r = r + GAMMA_N * v * s_mask\t# set v to 0 where s_ is terminal state\n",
    "        \n",
    "        s_t, a_t, r_t, minimize = self.graph\n",
    "        self.session.run(minimize, feed_dict={s_t: s, a_t: a, r_t: r})\n",
    "\n",
    "    def train_push(self, s, a, r, s_):\n",
    "        with self.lock_queue:\n",
    "            self.train_queue[0].append(s)\n",
    "            self.train_queue[1].append(a)\n",
    "            self.train_queue[2].append(r)\n",
    "\n",
    "            if s_ is None:\n",
    "                self.train_queue[3].append(NONE_STATE)\n",
    "                self.train_queue[4].append(0.)\n",
    "            else:\t\n",
    "                self.train_queue[3].append(s_)\n",
    "                self.train_queue[4].append(1.)\n",
    "\n",
    "    def predict(self, s):\n",
    "        with self.default_graph.as_default():\n",
    "            p, v = self.model.predict(s)\n",
    "            return p, v\n",
    "\n",
    "    def predict_p(self, s):\n",
    "        with self.default_graph.as_default():\n",
    "            p, v = self.model.predict(s)\t\t\n",
    "            return p\n",
    "\n",
    "    def predict_v(self, s):\n",
    "        with self.default_graph.as_default():\n",
    "            p, v = self.model.predict(s)\t\t\n",
    "            return v\n",
    "\n",
    "#---------\n",
    "frames = 0\n",
    "class Agent:\n",
    "    def __init__(self, eps_start, eps_end, eps_steps):\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end   = eps_end\n",
    "        self.eps_steps = eps_steps\n",
    "\n",
    "        self.memory = []\t# used for n_step return\n",
    "        self.R = 0.\n",
    "\n",
    "    def getEpsilon(self):\n",
    "        if(frames >= self.eps_steps):\n",
    "            return self.eps_end\n",
    "        else:\n",
    "            return self.eps_start + frames * (self.eps_end - self.eps_start) / self.eps_steps\t# linearly interpolate\n",
    "\n",
    "    def act(self, s):\n",
    "        eps = self.getEpsilon()\t\t\t\n",
    "        global frames; frames = frames + 1\n",
    "\n",
    "        if random.random() < eps:\n",
    "            return random.randint(0, NUM_ACTIONS-1)\n",
    "\n",
    "        else:\n",
    "            s = np.array([s])\n",
    "            p = brain.predict_p(s)[0]\n",
    "\n",
    "            # a = np.argmax(p)\n",
    "            a = np.random.choice(NUM_ACTIONS, p=p)\n",
    "\n",
    "            return a\n",
    "    \n",
    "    def train(self, s, a, r, s_):\n",
    "        def get_sample(memory, n):\n",
    "            s, a, _, _  = memory[0]\n",
    "            _, _, _, s_ = memory[n-1]\n",
    "\n",
    "            return s, a, self.R, s_\n",
    "\n",
    "        a_cats = np.zeros(NUM_ACTIONS)\t# turn action into one-hot representation\n",
    "        a_cats[a] = 1 \n",
    "\n",
    "        self.memory.append( (s, a_cats, r, s_) )\n",
    "\n",
    "        self.R = ( self.R + r * GAMMA_N ) / GAMMA\n",
    "\n",
    "        if s_ is None:\n",
    "            while len(self.memory) > 0:\n",
    "                n = len(self.memory)\n",
    "                s, a, r, s_ = get_sample(self.memory, n)\n",
    "                brain.train_push(s, a, r, s_)\n",
    "\n",
    "                self.R = ( self.R - self.memory[0][2] ) / GAMMA\n",
    "                self.memory.pop(0)\t\t\n",
    "\n",
    "            self.R = 0\n",
    "\n",
    "        if len(self.memory) >= N_STEP_RETURN:\n",
    "            s, a, r, s_ = get_sample(self.memory, N_STEP_RETURN)\n",
    "            brain.train_push(s, a, r, s_)\n",
    "\n",
    "            self.R = self.R - self.memory[0][2]\n",
    "            self.memory.pop(0)\t\n",
    "\n",
    "    # possible edge case - if an episode ends in <N steps, the computation is incorrect\n",
    "\n",
    "#---------\n",
    "class Environment(threading.Thread):\n",
    "    stop_signal = False\n",
    "\n",
    "    def __init__(self, render=False, eps_start=EPS_START, eps_end=EPS_STOP, eps_steps=EPS_STEPS):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        self.render = render\n",
    "        self.env = gym.make(ENV)\n",
    "        self.agent = Agent(eps_start, eps_end, eps_steps)\n",
    "\n",
    "    def runEpisode(self):\n",
    "        s = self.env.reset()\n",
    "\n",
    "        R = 0\n",
    "        while True:         \n",
    "            time.sleep(THREAD_DELAY) # yield \n",
    "\n",
    "            if self.render: self.env.render()\n",
    "\n",
    "            a = self.agent.act(s)\n",
    "            s_, r, done, info = self.env.step(a)\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            self.agent.train(s, a, r, s_)\n",
    "\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            if done or self.stop_signal:\n",
    "                break\n",
    "\n",
    "        print(\"Total R:\", R)\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            self.runEpisode()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "#---------\n",
    "class Optimizer(threading.Thread):\n",
    "    stop_signal = False\n",
    "\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            brain.optimize()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "#-- main\n",
    "env_test = Environment(render=True, eps_start=0., eps_end=0.)\n",
    "NUM_STATE = env_test.env.observation_space.shape[0]\n",
    "NUM_ACTIONS = env_test.env.action_space.n\n",
    "NONE_STATE = np.zeros(NUM_STATE)\n",
    "\n",
    "brain = Brain()\t# brain is global in A3C\n",
    "\n",
    "envs = [Environment() for i in range(THREADS)]\n",
    "opts = [Optimizer() for i in range(OPTIMIZERS)]\n",
    "\n",
    "for o in opts:\n",
    "    o.start()\n",
    "\n",
    "for e in envs:\n",
    "    e.start()\n",
    "\n",
    "for i in range(0, RUN_TIME):\n",
    "    time.sleep(1)\n",
    "    print(\"Elapsed time(s): \", i)\n",
    "\n",
    "for e in envs:\n",
    "    e.stop()\n",
    "for e in envs:\n",
    "    e.join()\n",
    "\n",
    "for o in opts:\n",
    "    o.stop()\n",
    "for o in opts:\n",
    "    o.join()\n",
    "\n",
    "print(\"Training finished\")\n",
    "env_test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
