{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "from gym import error, spaces\n",
    "from gym import utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "try:\n",
    "    import atari_py\n",
    "except ImportError as e:\n",
    "    raise error.DependencyNotInstalled(\"{}. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def eps_greedy_policy(q_values, eps):\n",
    "    '''\n",
    "    Creates an epsilon-greedy policy\n",
    "    :param q_values: set of Q-values of shape (num actions,)\n",
    "    :param eps: probability of taking a uniform random action \n",
    "    :return: policy of shape (num actions,)\n",
    "    '''\n",
    "\n",
    "    policy = np.zeros_like(q_values)\n",
    "    r = random.uniform(0,1)\n",
    "    \n",
    "    if(r < eps):\n",
    "        policy[:] = 1./len(q_values)\n",
    "    else:\n",
    "        greedy_action = np.argmax(q_values)\n",
    "        policy[greedy_action] = 1.\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def calculate_td_targets(q1_batch, q2_batch, r_batch, t_batch, gamma=.99):\n",
    "    '''\n",
    "    Calculates the TD-target used for the loss\n",
    "    : param q1_batch: Batch of Q(s', a) from online network, shape (N, num actions)\n",
    "    : param q2_batch: Batch of Q(s', a) from target network, shape (N, num actions)\n",
    "    : param r_batch: Batch of rewards, shape (N, 1)\n",
    "    : param t_batch: Batch of booleans indicating if state, s' is terminal, shape (N, 1)\n",
    "    : return: TD-target, shape (N, 1)\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    Y = np.zeros_like(r_batch)\n",
    "    \n",
    "    for i,(q1, q2, r, t) in enumerate(zip(q1_batch, q2_batch, r_batch, t_batch)):\n",
    "        Y[i] = r\n",
    "        if not t: Y[i] += gamma * q2[np.argmax(q1)]\n",
    "        \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ram(ale):\n",
    "    ram_size = ale.getRAMSize()\n",
    "    ram = np.zeros((ram_size),dtype=np.uint8)\n",
    "    ale.getRAM(ram)\n",
    "    return ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from keras.utils.np_utils import to_categorical as one_hot\n",
    "from collections import namedtuple\n",
    "from dqn_model import DoubleQLearningModel, ExperienceReplay\n",
    "\n",
    "def train_loop_ddqn(model, env, num_episodes, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() #reset to initial state\n",
    "        state = np.expand_dims(state, axis=0)/2\n",
    "        terminal = False # reset terminal flag\n",
    "        ep_reward = 0\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not terminal:\n",
    "            env.render() # comment this line out if jous' don't want to render the environment\n",
    "            steps += 1\n",
    "            q_values = model.get_q_values(state)\n",
    "            q_buffer.append(q_values)\n",
    "            policy = eps_greedy_policy(q_values.squeeze(), eps) \n",
    "            action = np.random.choice(num_actions, p=policy) # sample action from epsilon-greedy policy\n",
    "            new_state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "            new_state = np.expand_dims(new_state, axis=0)/2\n",
    "            \n",
    "            # only use the terminal flag for ending the episode and not for training\n",
    "            # if the flag is set due to that the maximum amount of steps is reached \n",
    "            t_to_buffer = terminal if not steps == 200 else False\n",
    "            \n",
    "            # store data to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=action, r=reward, next_s=new_state, t=t_to_buffer))\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # if buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                s, a, r, s_, t = replay_buffer.sample_minibatch(batch_size) # sample a minibatch of transitions\n",
    "                q_1, q_2 = model.get_q_values_for_both_models(np.squeeze(s_))\n",
    "                td_target = calculate_td_targets(q_1, q_2, r, t, gamma)\n",
    "                model.update(s, td_target, a)    \n",
    "                \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # running average of episodic rewards\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "        print('Episode: ', i, 'Reward:', ep_reward, 'Epsilon', eps, 'mean q', np.mean(np.array(q_buffer)))\n",
    "        \n",
    "        # if running average > 195, the task is considerd solved\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariEnv(gym.Env, utils.EzPickle):\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "\n",
    "    def __init__(self, game='SpaceInvaders-v0', obs_type='ram', frameskip=(2, 5), repeat_action_probability=0.):\n",
    "        \"\"\"Frameskip should be either a tuple (indicating a random range to\n",
    "        choose from, with the top value exclude), or an int.\"\"\"\n",
    "\n",
    "        utils.EzPickle.__init__(self, game, obs_type, frameskip, repeat_action_probability)\n",
    "        assert obs_type in ('ram', 'image')\n",
    "\n",
    "        self.game_path = atari_py.get_game_path(game)\n",
    "        if not os.path.exists(self.game_path):\n",
    "            raise IOError('You asked for game %s but path %s does not exist'%(game, self.game_path))\n",
    "        self._obs_type = obs_type\n",
    "        self.frameskip = frameskip\n",
    "        self.ale = atari_py.ALEInterface()\n",
    "        self.viewer = None\n",
    "\n",
    "        # Tune (or disable) ALE's action repeat:\n",
    "        # https://github.com/openai/gym/issues/349\n",
    "        assert isinstance(repeat_action_probability, (float, int)), \"Invalid repeat_action_probability: {!r}\".format(repeat_action_probability)\n",
    "        self.ale.setFloat('repeat_action_probability'.encode('utf-8'), repeat_action_probability)\n",
    "\n",
    "        self.seed()\n",
    "\n",
    "        self._action_set = self.ale.getMinimalActionSet()\n",
    "        self.action_space = spaces.Discrete(len(self._action_set))\n",
    "\n",
    "        (screen_width,screen_height) = self.ale.getScreenDims()\n",
    "        if self._obs_type == 'ram':\n",
    "            self.observation_space = spaces.Box(low=0, high=255, dtype=np.uint8, shape=(128,))\n",
    "        elif self._obs_type == 'image':\n",
    "            self.observation_space = spaces.Box(low=0, high=255, shape=(screen_height, screen_width, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            raise error.Error('Unrecognized observation type: {}'.format(self._obs_type))\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed1 = seeding.np_random(seed)\n",
    "        # Derive a random seed. This gets passed as a uint, but gets\n",
    "        # checked as an int elsewhere, so we need to keep it below\n",
    "        # 2**31.\n",
    "        seed2 = seeding.hash_seed(seed1 + 1) % 2**31\n",
    "        # Empirically, we need to seed before loading the ROM.\n",
    "        self.ale.setInt(b'random_seed', seed2)\n",
    "        self.ale.loadROM(self.game_path)\n",
    "        return [seed1, seed2]\n",
    "\n",
    "    def step(self, a):\n",
    "        reward = 0.0\n",
    "        action = self._action_set[a]\n",
    "\n",
    "        if isinstance(self.frameskip, int):\n",
    "            num_steps = self.frameskip\n",
    "        else:\n",
    "            num_steps = self.np_random.randint(self.frameskip[0], self.frameskip[1])\n",
    "        for _ in range(num_steps):\n",
    "            reward += self.ale.act(action)\n",
    "        ob = self._get_obs()\n",
    "\n",
    "        return ob, reward, self.ale.game_over(), {\"ale.lives\": self.ale.lives()}\n",
    "\n",
    "    def _get_image(self):\n",
    "        return self.ale.getScreenRGB2()\n",
    "\n",
    "    def _get_ram(self):\n",
    "        return to_ram(self.ale)\n",
    "\n",
    "    @property\n",
    "    def _n_actions(self):\n",
    "        return len(self._action_set)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self._obs_type == 'ram':\n",
    "            return self._get_ram()\n",
    "        elif self._obs_type == 'image':\n",
    "            img = self._get_image()\n",
    "        return img\n",
    "\n",
    "    # return: (states, observations)\n",
    "    def reset(self):\n",
    "        self.ale.reset_game()\n",
    "        return self._get_obs()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        img = self._get_image()\n",
    "        if mode == 'rgb_array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            from gym.envs.classic_control import rendering\n",
    "            if self.viewer is None:\n",
    "                self.viewer = rendering.SimpleImageViewer()\n",
    "            self.viewer.imshow(img)\n",
    "            return self.viewer.isopen\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "    def get_action_meanings(self):\n",
    "        return [ACTION_MEANING[i] for i in self._action_set]\n",
    "\n",
    "    def get_keys_to_action(self):\n",
    "        KEYWORD_TO_KEY = {\n",
    "            'UP':      ord('w'),\n",
    "            'DOWN':    ord('s'),\n",
    "            'LEFT':    ord('a'),\n",
    "            'RIGHT':   ord('d'),\n",
    "            'FIRE':    ord(' '),\n",
    "        }\n",
    "\n",
    "        keys_to_action = {}\n",
    "\n",
    "        for action_id, action_meaning in enumerate(self.get_action_meanings()):\n",
    "            keys = []\n",
    "            for keyword, key in KEYWORD_TO_KEY.items():\n",
    "                if keyword in action_meaning:\n",
    "                    keys.append(key)\n",
    "            keys = tuple(sorted(keys))\n",
    "\n",
    "            assert keys not in keys_to_action\n",
    "            keys_to_action[keys] = action_id\n",
    "\n",
    "        return keys_to_action\n",
    "\n",
    "    def clone_state(self):\n",
    "        \"\"\"Clone emulator state w/o system state. Restoring this state will\n",
    "        *not* give an identical environment. For complete cloning and restoring\n",
    "        of the full state, see `{clone,restore}_full_state()`.\"\"\"\n",
    "        state_ref = self.ale.cloneState()\n",
    "        state = self.ale.encodeState(state_ref)\n",
    "        self.ale.deleteState(state_ref)\n",
    "        return state\n",
    "\n",
    "    def restore_state(self, state):\n",
    "        \"\"\"Restore emulator state w/o system state.\"\"\"\n",
    "        state_ref = self.ale.decodeState(state)\n",
    "        self.ale.restoreState(state_ref)\n",
    "        self.ale.deleteState(state_ref)\n",
    "\n",
    "    def clone_full_state(self):\n",
    "        \"\"\"Clone emulator state w/ system state including pseudorandomness.\n",
    "        Restoring this state will give an identical environment.\"\"\"\n",
    "        state_ref = self.ale.cloneSystemState()\n",
    "        state = self.ale.encodeState(state_ref)\n",
    "        self.ale.deleteState(state_ref)\n",
    "        return state\n",
    "\n",
    "    def restore_full_state(self, state):\n",
    "        \"\"\"Restore emulator state w/ system state including pseudorandomness.\"\"\"\n",
    "        state_ref = self.ale.decodeState(state)\n",
    "        self.ale.restoreSystemState(state_ref)\n",
    "        self.ale.deleteState(state_ref)\n",
    "\n",
    "ACTION_MEANING = {\n",
    "    0 : \"NOOP\",\n",
    "    1 : \"FIRE\",\n",
    "    2 : \"UP\",\n",
    "    3 : \"RIGHT\",\n",
    "    4 : \"LEFT\",\n",
    "    5 : \"DOWN\",\n",
    "    6 : \"UPRIGHT\",\n",
    "    7 : \"UPLEFT\",\n",
    "    8 : \"DOWNRIGHT\",\n",
    "    9 : \"DOWNLEFT\",\n",
    "    10 : \"UPFIRE\",\n",
    "    11 : \"RIGHTFIRE\",\n",
    "    12 : \"LEFTFIRE\",\n",
    "    13 : \"DOWNFIRE\",\n",
    "    14 : \"UPRIGHTFIRE\",\n",
    "    15 : \"UPLEFTFIRE\",\n",
    "    16 : \"DOWNRIGHTFIRE\",\n",
    "    17 : \"DOWNLEFTFIRE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 Reward: 55.0 Epsilon 0.999 mean q -2.4362838e-05\n",
      "Episode:  1 Reward: 200.0 Epsilon 0.998 mean q 2.1278098\n",
      "Episode:  2 Reward: 120.0 Epsilon 0.997 mean q 4.4097934\n",
      "Episode:  3 Reward: 315.0 Epsilon 0.996 mean q 4.4932804\n",
      "Episode:  4 Reward: 245.0 Epsilon 0.995 mean q 4.50747\n",
      "Episode:  5 Reward: 680.0 Epsilon 0.994 mean q 5.401476\n",
      "Episode:  6 Reward: 430.0 Epsilon 0.993 mean q 5.8680496\n",
      "Episode:  7 Reward: 260.0 Epsilon 0.992 mean q 5.0159574\n",
      "Episode:  8 Reward: 30.0 Epsilon 0.991 mean q 3.367363\n",
      "Episode:  9 Reward: 155.0 Epsilon 0.99 mean q 5.162736\n",
      "Episode:  10 Reward: 65.0 Epsilon 0.989 mean q 5.174258\n",
      "Episode:  11 Reward: 230.0 Epsilon 0.988 mean q 6.429678\n",
      "Episode:  12 Reward: 225.0 Epsilon 0.987 mean q 5.3131666\n",
      "Episode:  13 Reward: 80.0 Epsilon 0.986 mean q 4.1937556\n",
      "Episode:  14 Reward: 385.0 Epsilon 0.985 mean q 5.631878\n",
      "Episode:  15 Reward: 210.0 Epsilon 0.984 mean q 4.3702636\n",
      "Episode:  16 Reward: 80.0 Epsilon 0.983 mean q 4.6784205\n",
      "Episode:  17 Reward: 170.0 Epsilon 0.982 mean q 5.656258\n",
      "Episode:  18 Reward: 465.0 Epsilon 0.981 mean q 6.0590777\n",
      "Episode:  19 Reward: 315.0 Epsilon 0.98 mean q 6.700167\n",
      "Episode:  20 Reward: 105.0 Epsilon 0.979 mean q 5.6253242\n",
      "Episode:  21 Reward: 75.0 Epsilon 0.978 mean q 5.0865674\n",
      "Episode:  22 Reward: 45.0 Epsilon 0.977 mean q 2.592658\n",
      "Episode:  23 Reward: 120.0 Epsilon 0.976 mean q 6.3224626\n",
      "Episode:  24 Reward: 210.0 Epsilon 0.975 mean q 5.7214804\n",
      "Episode:  25 Reward: 120.0 Epsilon 0.974 mean q 5.798181\n",
      "Episode:  26 Reward: 45.0 Epsilon 0.973 mean q 3.9040377\n",
      "Episode:  27 Reward: 435.0 Epsilon 0.972 mean q 7.2611513\n",
      "Episode:  28 Reward: 95.0 Epsilon 0.971 mean q 5.7472982\n",
      "Episode:  29 Reward: 125.0 Epsilon 0.97 mean q 3.7732718\n",
      "Episode:  30 Reward: 35.0 Epsilon 0.969 mean q 3.5996046\n",
      "Episode:  31 Reward: 80.0 Epsilon 0.968 mean q 5.8677287\n",
      "Episode:  32 Reward: 55.0 Epsilon 0.967 mean q 4.861308\n",
      "Episode:  33 Reward: 80.0 Epsilon 0.966 mean q 3.3857026\n",
      "Episode:  34 Reward: 75.0 Epsilon 0.965 mean q 5.757714\n",
      "Episode:  35 Reward: 35.0 Epsilon 0.964 mean q 4.3135853\n",
      "Episode:  36 Reward: 105.0 Epsilon 0.963 mean q 4.255835\n",
      "Episode:  37 Reward: 135.0 Epsilon 0.962 mean q 4.4393096\n",
      "Episode:  38 Reward: 55.0 Epsilon 0.961 mean q 3.4247105\n",
      "Episode:  39 Reward: 105.0 Epsilon 0.96 mean q 3.892196\n",
      "Episode:  40 Reward: 410.0 Epsilon 0.959 mean q 5.5877714\n",
      "Episode:  41 Reward: 350.0 Epsilon 0.958 mean q 4.0861864\n",
      "Episode:  42 Reward: 85.0 Epsilon 0.957 mean q 4.6824803\n",
      "Episode:  43 Reward: 180.0 Epsilon 0.956 mean q 4.655531\n",
      "Episode:  44 Reward: 85.0 Epsilon 0.955 mean q 3.5059335\n",
      "Episode:  45 Reward: 45.0 Epsilon 0.954 mean q 3.8028872\n",
      "Episode:  46 Reward: 210.0 Epsilon 0.953 mean q 3.4920967\n",
      "Episode:  47 Reward: 180.0 Epsilon 0.952 mean q 4.389181\n",
      "Episode:  48 Reward: 80.0 Epsilon 0.951 mean q 3.5832958\n",
      "Episode:  49 Reward: 160.0 Epsilon 0.95 mean q 4.360513\n",
      "Episode:  50 Reward: 30.0 Epsilon 0.949 mean q 2.5872629\n",
      "Episode:  51 Reward: 85.0 Epsilon 0.948 mean q 4.7324624\n",
      "Episode:  52 Reward: 410.0 Epsilon 0.947 mean q 4.3454456\n",
      "Episode:  53 Reward: 195.0 Epsilon 0.946 mean q 5.577513\n",
      "Episode:  54 Reward: 105.0 Epsilon 0.945 mean q 2.2029624\n",
      "Episode:  55 Reward: 110.0 Epsilon 0.944 mean q 6.17937\n",
      "Episode:  56 Reward: 65.0 Epsilon 0.943 mean q 3.609692\n",
      "Episode:  57 Reward: 190.0 Epsilon 0.942 mean q 4.0260415\n",
      "Episode:  58 Reward: 55.0 Epsilon 0.941 mean q 3.2391272\n",
      "Episode:  59 Reward: 35.0 Epsilon 0.94 mean q 2.6241357\n",
      "Episode:  60 Reward: 495.0 Epsilon 0.939 mean q 4.7677383\n",
      "Episode:  61 Reward: 275.0 Epsilon 0.938 mean q 4.8180184\n",
      "Episode:  62 Reward: 260.0 Epsilon 0.9369999999999999 mean q 5.613576\n",
      "Episode:  63 Reward: 45.0 Epsilon 0.9359999999999999 mean q 3.1545794\n",
      "Episode:  64 Reward: 460.0 Epsilon 0.9349999999999999 mean q 4.5805817\n",
      "Episode:  65 Reward: 505.0 Epsilon 0.9339999999999999 mean q 6.2214217\n",
      "Episode:  66 Reward: 180.0 Epsilon 0.9329999999999999 mean q 6.0353208\n",
      "Episode:  67 Reward: 100.0 Epsilon 0.9319999999999999 mean q 3.0381818\n",
      "Episode:  68 Reward: 445.0 Epsilon 0.9309999999999999 mean q 3.9701798\n",
      "Episode:  69 Reward: 245.0 Epsilon 0.9299999999999999 mean q 6.0091667\n",
      "Episode:  70 Reward: 135.0 Epsilon 0.9289999999999999 mean q 4.4684505\n",
      "Episode:  71 Reward: 95.0 Epsilon 0.9279999999999999 mean q 4.443172\n",
      "Episode:  72 Reward: 120.0 Epsilon 0.9269999999999999 mean q 3.0167913\n",
      "Episode:  73 Reward: 105.0 Epsilon 0.9259999999999999 mean q 3.479891\n",
      "Episode:  74 Reward: 315.0 Epsilon 0.9249999999999999 mean q 6.325807\n",
      "Episode:  75 Reward: 120.0 Epsilon 0.9239999999999999 mean q 3.5204563\n",
      "Episode:  76 Reward: 125.0 Epsilon 0.9229999999999999 mean q 4.9557376\n",
      "Episode:  77 Reward: 140.0 Epsilon 0.9219999999999999 mean q 4.2951436\n",
      "Episode:  78 Reward: 75.0 Epsilon 0.9209999999999999 mean q 4.0992284\n",
      "Episode:  79 Reward: 50.0 Epsilon 0.9199999999999999 mean q 3.453643\n",
      "Episode:  80 Reward: 10.0 Epsilon 0.9189999999999999 mean q 2.2279823\n",
      "Episode:  81 Reward: 30.0 Epsilon 0.9179999999999999 mean q 2.6701245\n",
      "Episode:  82 Reward: 205.0 Epsilon 0.9169999999999999 mean q 5.3631186\n",
      "Episode:  83 Reward: 125.0 Epsilon 0.9159999999999999 mean q 5.1919403\n",
      "Episode:  84 Reward: 95.0 Epsilon 0.9149999999999999 mean q 4.3666954\n",
      "Episode:  85 Reward: 125.0 Epsilon 0.9139999999999999 mean q 3.4203358\n",
      "Episode:  86 Reward: 90.0 Epsilon 0.9129999999999999 mean q 4.0114655\n",
      "Episode:  87 Reward: 145.0 Epsilon 0.9119999999999999 mean q 2.8019576\n",
      "Episode:  88 Reward: 80.0 Epsilon 0.9109999999999999 mean q 4.084586\n",
      "Episode:  89 Reward: 160.0 Epsilon 0.9099999999999999 mean q 4.365183\n",
      "Episode:  90 Reward: 245.0 Epsilon 0.9089999999999999 mean q 6.002739\n",
      "Episode:  91 Reward: 155.0 Epsilon 0.9079999999999999 mean q 3.8572981\n",
      "Episode:  92 Reward: 315.0 Epsilon 0.9069999999999999 mean q 3.3958437\n",
      "Episode:  93 Reward: 210.0 Epsilon 0.9059999999999999 mean q 5.7151875\n",
      "Episode:  94 Reward: 295.0 Epsilon 0.9049999999999999 mean q 5.8243427\n",
      "Episode:  95 Reward: 165.0 Epsilon 0.9039999999999999 mean q 3.1807613\n",
      "Episode:  96 Reward: 110.0 Epsilon 0.9029999999999999 mean q 3.7425342\n",
      "Episode:  97 Reward: 140.0 Epsilon 0.9019999999999999 mean q 4.2526345\n",
      "Episode:  98 Reward: 130.0 Epsilon 0.9009999999999999 mean q 3.7640443\n",
      "Episode:  99 Reward: 215.0 Epsilon 0.8999999999999999 mean q 3.5597043\n",
      "Episode:  100 Reward: 105.0 Epsilon 0.8989999999999999 mean q 3.9910278\n",
      "Episode:  101 Reward: 105.0 Epsilon 0.8979999999999999 mean q 3.9690845\n",
      "Episode:  102 Reward: 55.0 Epsilon 0.8969999999999999 mean q 2.050849\n",
      "Episode:  103 Reward: 125.0 Epsilon 0.8959999999999999 mean q 4.473257\n",
      "Episode:  104 Reward: 285.0 Epsilon 0.8949999999999999 mean q 3.7457533\n",
      "Episode:  105 Reward: 155.0 Epsilon 0.8939999999999999 mean q 4.1086745\n",
      "Episode:  106 Reward: 215.0 Epsilon 0.8929999999999999 mean q 5.40402\n",
      "Episode:  107 Reward: 120.0 Epsilon 0.8919999999999999 mean q 3.3467205\n",
      "Episode:  108 Reward: 210.0 Epsilon 0.8909999999999999 mean q 4.3429694\n",
      "Episode:  109 Reward: 85.0 Epsilon 0.8899999999999999 mean q 3.41125\n",
      "Episode:  110 Reward: 125.0 Epsilon 0.8889999999999999 mean q 3.6103246\n",
      "Episode:  111 Reward: 105.0 Epsilon 0.8879999999999999 mean q 3.0803893\n",
      "Episode:  112 Reward: 110.0 Epsilon 0.8869999999999999 mean q 3.7538996\n",
      "Episode:  113 Reward: 140.0 Epsilon 0.8859999999999999 mean q 3.4772477\n",
      "Episode:  114 Reward: 115.0 Epsilon 0.8849999999999999 mean q 3.8757617\n",
      "Episode:  115 Reward: 80.0 Epsilon 0.8839999999999999 mean q 3.287724\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"SpaceInvaders-ram-v0\")\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Our Neural Netork model used to estimate the Q-values\n",
    "model = DoubleQLearningModel(state_dim=obs_dim, action_dim=num_actions, learning_rate=1e-4)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(state_size=obs_dim)\n",
    "\n",
    "# Train\n",
    "num_episodes = 1200 \n",
    "batch_size = 128 \n",
    "R, R_avg = train_loop_ddqn(model, env, num_episodes, batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_episodes = 1\n",
    "env = gym.make(\"SpaceInvaders-ram-v0\")\n",
    "\n",
    "for i in range(num_episodes):\n",
    "        state = env.reset() #reset to initial state\n",
    "        state = np.expand_dims(state, axis=0)/2\n",
    "        terminal = False # reset terminal flag\n",
    "        while not terminal:\n",
    "            env.render()\n",
    "            time.sleep(.05)\n",
    "            q_values = model.get_q_values(state)\n",
    "            policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "            action = np.random.choice(num_actions, p=policy)\n",
    "            state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "            state = np.expand_dims(state, axis=0)/2\n",
    "# close window\n",
    "env.close();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
